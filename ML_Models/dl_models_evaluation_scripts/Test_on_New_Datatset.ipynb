{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"MUh8Bahf8U6z"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"_siLeHBw7_Mm"},"outputs":[],"source":["dfTotal_external = pd.read_csv('../../Datasets/External_Validation_Datasets/External_Validation2.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655592305395,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"kNbRRMUa8o-C","outputId":"572004f0-c06d-45b5-d81f-3e0f67de29cf"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-f093e7d2-3a01-4c01-8d8c-87ff34ec8379\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Origins: Hunting the Source of Covid-19</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Totally Under Control: Trump and Covid-19</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Newsday: Mass Covid-19 testing in Beijing</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Covid-19 in the UK</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Coronavirus: More than 100 TfL workers died fr...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>30625</th>\n","      <td>After COVID-19 outbreak in europe Muslims and...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30626</th>\n","      <td>Saddam Hussein predicted the coronavirus outb...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30627</th>\n","      <td>Eight COVID-19 patients in Ghana have recovered.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30628</th>\n","      <td>People with type A blood are more prone to ge...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30629</th>\n","      <td>There’s definitive proof that chloroquine, hy...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>30630 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f093e7d2-3a01-4c01-8d8c-87ff34ec8379')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f093e7d2-3a01-4c01-8d8c-87ff34ec8379 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f093e7d2-3a01-4c01-8d8c-87ff34ec8379');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                   title  label\n","0                Origins: Hunting the Source of Covid-19      1\n","1              Totally Under Control: Trump and Covid-19      1\n","2              Newsday: Mass Covid-19 testing in Beijing      1\n","3                                     Covid-19 in the UK      1\n","4      Coronavirus: More than 100 TfL workers died fr...      1\n","...                                                  ...    ...\n","30625   After COVID-19 outbreak in europe Muslims and...      0\n","30626   Saddam Hussein predicted the coronavirus outb...      0\n","30627   Eight COVID-19 patients in Ghana have recovered.      0\n","30628   People with type A blood are more prone to ge...      0\n","30629   There’s definitive proof that chloroquine, hy...      0\n","\n","[30630 rows x 2 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dfTotal_external"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4976,"status":"ok","timestamp":1655592310509,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"UAb5d0IR857u","outputId":"78358714-4db9-4b0c-e9f0-36be630e15a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n","    return self.__dep_map\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n","    raise AttributeError(attr)\n","AttributeError: _DistInfoDistribution__dep_map\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n","    status = self.run(options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n","    conflicts = self._determine_conflicts(to_install)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n","    return check_install_conflicts(to_install)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n","    package_set, _ = create_package_set_from_installed()\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n","    package_set[name] = PackageDetails(dist.version, dist.requires())\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n","    dm = self._dep_map\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n","    self.__dep_map = self._compute_dependencies()\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3045, in _compute_dependencies\n","    dm[s_extra] = list(frozenset(reqs_for_extra(extra)) - common)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3037, in reqs_for_extra\n","    if not req.marker or req.marker.evaluate({'extra': extra}):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/markers.py\", line 336, in evaluate\n","    return _evaluate_markers(self._markers, current_environment)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 212, in _main\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1425, in critical\n","    self._log(CRITICAL, msg, args, **kwargs)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1502, in _log\n","    fn, lno, func, sinfo = self.findCaller(stack_info)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1451, in findCaller\n","    f = currentframe()\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 154, in <lambda>\n","    currentframe = lambda: sys._getframe(3)\n","KeyboardInterrupt\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.3.2)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ztjffqHlXlM"},"outputs":[],"source":["model_path = '../../ML_Models/models/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbCsY25B82OK"},"outputs":[],"source":["from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"]},{"cell_type":"markdown","metadata":{"id":"MwpKjHAB8az7"},"source":["# Validate on BERT"]},{"cell_type":"markdown","metadata":{"id":"aAA6N_G_94eT"},"source":["## Trained on FNN&CoAID"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23739,"status":"ok","timestamp":1655592337081,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"awFHhnWw8Tk-","outputId":"fa706697-1d4e-414f-bb52-3c0cf6d088ba"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","import pandas as pd\n","from datasets import Dataset\n","import numpy as np\n","from datasets import load_metric\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n","\n","tokenizer(\"Attention is all you need\")\n","\n","def tokenize_data(example):\n","    return tokenizer(example['title'], padding='max_length')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400,"referenced_widgets":["28955720ae3f4c89b4ae09c393b24c10","8b76dcf719534665a75d375be2428405","7a5e4a6f44c54a8bb249030be6380ec5","80cab5c8fd9e4e67b51ec61b542deba9","11f96e5bffed448385841900683d95fe","ba6c763e1f3148acbe647564fb632dcd","897e7b88bc6d4f6e82158742a3f9cb68","0c6e5d752fa8429c9080dc9334abbe01","30ac56e6f1fa4312a0f625f0452e0b91","784d6a63273a49558e0af472da52e53a","f040058a407a48dd87871fe1dc4c0c30"]},"executionInfo":{"elapsed":579616,"status":"ok","timestamp":1655585080943,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"Xnvl8Nca84cX","outputId":"b0677293-afb9-4b1d-9925-b4b37436d784"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28955720ae3f4c89b4ae09c393b24c10","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:30]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.8674502122102514,\n"," 'eval_f1': 0.8734650626441438,\n"," 'eval_loss': 0.5516829490661621,\n"," 'eval_precision': 0.883877885707077,\n"," 'eval_recall': 0.8632947264662395,\n"," 'eval_runtime': 570.4063,\n"," 'eval_samples_per_second': 53.699,\n"," 'eval_steps_per_second': 6.713}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","from datasets import load_metric\n","from transformers import Trainer\n","from transformers import TrainingArguments\n","\n","#Load saved model\n","import torch\n","model.load_state_dict(torch.load(model_path+'finetune_bert_on_CoAID_FNN.pt'))\n","\n","#Dataset\n","dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)\n","\n","#Eval Parameters\n","metric = load_metric(\"accuracy\")\n","test_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"m7Zv7S-4_ie9"},"source":["## Out of box"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":569756,"status":"ok","timestamp":1655585650692,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"Z2BanEDG_hC_","outputId":"4fc8835d-e18c-40a7-e7f5-2b928e51927e"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:27]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"data":{"text/plain":["{'eval_accuracy': 0.47006203068886715,\n"," 'eval_f1': 0.0,\n"," 'eval_loss': 0.7524212002754211,\n"," 'eval_precision': 0.0,\n"," 'eval_recall': 0.0,\n"," 'eval_runtime': 567.5186,\n"," 'eval_samples_per_second': 53.972,\n"," 'eval_steps_per_second': 6.747}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"RHT4xiF99-Vb"},"source":["## finetune_bert_on_CoAID&GossipCop_only"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":289963,"status":"ok","timestamp":1655587879051,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"s0aWqvP_925G","outputId":"74656c57-0ac8-4ce3-c97d-7eeb623bc0e2"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1873' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1873/3829 04:36 < 04:48, 6.77 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:26]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.9047012732615083,\n"," 'eval_f1': 0.9109599487539274,\n"," 'eval_loss': 0.4938521087169647,\n"," 'eval_precision': 0.9021811370914145,\n"," 'eval_recall': 0.9199112863479546,\n"," 'eval_runtime': 566.8569,\n"," 'eval_samples_per_second': 54.035,\n"," 'eval_steps_per_second': 6.755}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_CoAID&GossipCop_only.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"jpzFJI7Ynb0j"},"source":["## CoAID "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":574639,"status":"ok","timestamp":1655588453688,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"iVDlKcZWnd7y","outputId":"02d4e0d4-3b16-4dec-f3de-3980f15b0dae"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:28]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.9455762324518446,\n"," 'eval_f1': 0.9498088098033902,\n"," 'eval_loss': 0.336826890707016,\n"," 'eval_precision': 0.9288616689240916,\n"," 'eval_recall': 0.9717225234105471,\n"," 'eval_runtime': 568.4018,\n"," 'eval_samples_per_second': 53.888,\n"," 'eval_steps_per_second': 6.736}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_CoAID.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"-meGXc9Y-0cF"},"source":["## finetune_bert_on_CoAID&PolitiFact_only"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":92,"referenced_widgets":["477c1884e49d4afba83ac68fc9952bb4","45f163089f9b42e488722f11899b00e6","ca03003c9d51469cbb3cebd7b1f713a9","7ec597fdcb5c40cc925b66d2ac54901a","3bb2ab038c1144e3910979483e20d945","03a6d5047d99473ea60cedc61508ef1d","12d8f144439649f89e70e78e2f247bff","ea73752595ab4ad8aa493b97193b88b4","10d5aa36dfd3443787a7aeee5161b3c7","ca026651ce06469c8f757f5a8c4700de","6592309d4696478b9cf013628808b9c6"]},"executionInfo":{"elapsed":14430,"status":"ok","timestamp":1655592492797,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"JqrfHtlz_uWU","outputId":"6fa40323-95a6-4ddb-fd94-399f791659f1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Parameter 'function'=<function tokenize_data at 0x7f9faa148200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"477c1884e49d4afba83ac68fc9952bb4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","import numpy as np\n","from datasets import load_metric\n","from transformers import Trainer\n","from transformers import TrainingArguments\n","\n","\n","#Eval Parameters\n","metric = load_metric(\"accuracy\")\n","test_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n","\n","#Dataset\n","dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"executionInfo":{"elapsed":573055,"status":"ok","timestamp":1655593065846,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"HlF-0xTm-xlJ","outputId":"1fb58f86-d45d-4bf9-d12f-e4e70137b3a3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:30]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.9345086516487104,\n"," 'eval_f1': 0.9375622509960159,\n"," 'eval_loss': 0.35977908968925476,\n"," 'eval_precision': 0.9474710619023654,\n"," 'eval_recall': 0.92785855101035,\n"," 'eval_runtime': 570.2881,\n"," 'eval_samples_per_second': 53.71,\n"," 'eval_steps_per_second': 6.714}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_CoAID&PolitiFact_only.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"MqWbBiOm-8oF"},"source":["## finetune_bert_on_all_of_FNN.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":574563,"status":"ok","timestamp":1655593640402,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"LBmv9U_6-71w","outputId":"5b9a2ae4-4fd7-42e3-be68-3640d844a3fc"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:29]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.5354554358472087,\n"," 'eval_f1': 0.5653010723123453,\n"," 'eval_loss': 1.5617187023162842,\n"," 'eval_precision': 0.5606932913156778,\n"," 'eval_recall': 0.5699852143913258,\n"," 'eval_runtime': 569.7573,\n"," 'eval_samples_per_second': 53.76,\n"," 'eval_steps_per_second': 6.72}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_all_of_FNN.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"rE_pP9jp_Eeh"},"source":["## finetune_bert_on_GossipCop_only.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":573028,"status":"ok","timestamp":1655594213422,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"UZtFCtyz_Dwc","outputId":"9a30973a-d7a9-4e8f-c0cf-c859914fee64"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:28]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.40819458047665685,\n"," 'eval_f1': 0.4827507490369526,\n"," 'eval_loss': 1.9215482473373413,\n"," 'eval_precision': 0.4496358900760113,\n"," 'eval_recall': 0.5211310990635781,\n"," 'eval_runtime': 568.2586,\n"," 'eval_samples_per_second': 53.902,\n"," 'eval_steps_per_second': 6.738}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_GossipCop_only.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"mn3agbV__R6T"},"source":["##finetune_bert_on_PolitiFact_only"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":573206,"status":"ok","timestamp":1655594786619,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"0teMWm0R_Qmy","outputId":"d56543b3-6487-41d2-fefd-565f264333b0"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:28]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.841005550114267,\n"," 'eval_f1': 0.8362584896778966,\n"," 'eval_loss': 0.6823464035987854,\n"," 'eval_precision': 0.9205033308660252,\n"," 'eval_recall': 0.7661409561360276,\n"," 'eval_runtime': 568.4556,\n"," 'eval_samples_per_second': 53.883,\n"," 'eval_steps_per_second': 6.736}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["\n","import torch\n","#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_PolitiFact_only.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"Wv7BP-oL_YTz"},"source":["# Roberta-Fake-News"]},{"cell_type":"markdown","metadata":{"id":"YjCDKt2itCT_"},"source":["## 1.Out-of-box"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7558,"status":"ok","timestamp":1655594794167,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"xwoXTly9_Wly","outputId":"94155953-d68a-48d4-fba2-deb70c06dc32"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d4210e7379aab0e68db3bb082412960f0b90193184d6938abedc2212cffc8ee9.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/5d86bda5dc16a19ae0ebc44499ec825b06516f9aae277aff35d2ca814da2a5f1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/424977d8969812c880aa638f4451597de6d4c1c429d98b7aed0822f8c327bbcb.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0dc2cdacd2fe1dd988f8f9976e21db42274012f2d86da28d573081dbff2a059b.71056ac4e5c419bd472fc1a3f4269ad35695eb83fe663f969180490939da9f9c\n","loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]}],"source":["\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","def tokenize_data(example):\n","    return tokenizer(example['title'], padding='max_length')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400,"referenced_widgets":["d7abea0b567646d692309dffbe70d0c8","7c6acfcc13c346a1a0ae364c6d55bb3f","59a7c632cf55450996f0e2a7921404c1","c16ef68e7521495abf00badf2a99d2d6","95e5a5e414f542cfa0de676bc9369e28","60bf5603a1614e05b47e035a35cb3e3d","f99a96b6824347c78a0b661df7e52288","d578f2c9e4e64ab4b2c383889fcab4ea","3fa54875fdb34b568f0539ada8e4715c","3f230d91a1b8467eabca1f3137778647","db104488ce674ca6a1697f0bb7461e19"]},"executionInfo":{"elapsed":568397,"status":"ok","timestamp":1655595362556,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"CCtNU87ewfQF","outputId":"eda14f97-f4b0-4033-c4c8-620415437138"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7abea0b567646d692309dffbe70d0c8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:21]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.39268690825987596,\n"," 'eval_f1': 0.5589434749620638,\n"," 'eval_loss': 1.2812670469284058,\n"," 'eval_precision': 0.45432469935245146,\n"," 'eval_recall': 0.7261582060128142,\n"," 'eval_runtime': 561.9995,\n"," 'eval_samples_per_second': 54.502,\n"," 'eval_steps_per_second': 6.813}"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["#Dataset\n","dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)\n","\n","#Eval Parameters\n","metric = load_metric(\"accuracy\")\n","test_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"d7WvZAuNtVsU"},"source":["## 2.Finetuned on CoAID "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":50,"referenced_widgets":["86595ccf9a254d61b67f01cdbe26e73f","d6ddd167d85c4c48bb112081a86a1639","97485028605e458aada7c0663b3ccea1","843627041fc14ee2b647b4eabc4488e7","2460aba7842c44058707a3c87250ba40","5b3584c558254b898013e72e88ffd776","659a998199d84fdf929731b704ae79d6","5f0b0a259a0a4923ba92003237c85b65","bfad58c8124d4f8c95c98c4503871c07","8938483084aa4ca7a405b61995c0ccb8","c080e8bd441e4b3e9e626e0666b451e1"]},"executionInfo":{"elapsed":5690,"status":"ok","timestamp":1655595368220,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"dDWJ4EQQnTbQ","outputId":"ddaebc0a-470d-44f5-fb5d-d2dbdaffffee"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86595ccf9a254d61b67f01cdbe26e73f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":566179,"status":"ok","timestamp":1655595934386,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"n4XGccodtXT8","outputId":"00d70a8b-25e0-47c0-e2ae-9bee95c51cbb"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:21]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.8097616715638263,\n"," 'eval_f1': 0.797582241984229,\n"," 'eval_loss': 1.8956300020217896,\n"," 'eval_precision': 0.9143767423337316,\n"," 'eval_recall': 0.7072449482503697,\n"," 'eval_runtime': 561.2516,\n"," 'eval_samples_per_second': 54.574,\n"," 'eval_steps_per_second': 6.822}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_with_shuffle.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()\n"]},{"cell_type":"markdown","metadata":{"id":"IAFx2my9te5b"},"source":["## 3.Fine-tuned on FNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":50,"referenced_widgets":["9bd74301d6e943f49ebc7f8751bec137","0c08d5610f2b4ee99cc7c8827598e958","149dc4b93bf849e5bf09d33066a729fc","61c6adbff1fb47de87fe03187b00537c","cd52726c54c54878b01eebc00523401a","db8edd12161b425ea45a877c9bcfbbcd","4b4be4f8ca7f4a0287b2683dafad38ff","06290a60d2814d53af4c26d85d14b193","7914b8eb8fa347148c2282f856bbd6b0","0500ae8f031b4bde9833e40e94916336","b84463644c7d417d98002c8e03b8de80"]},"executionInfo":{"elapsed":5951,"status":"ok","timestamp":1655595940324,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"vCFx6YhOnVfM","outputId":"3abb2c06-03ee-47b0-8ac7-f81d16eb8c74"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9bd74301d6e943f49ebc7f8751bec137","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":570963,"status":"ok","timestamp":1655596511280,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"yhaU7px6tht2","outputId":"f83e9eff-536e-4240-f8d8-34fea9c729fe"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:25]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.4691478942213516,\n"," 'eval_f1': 0.5309795777085498,\n"," 'eval_loss': 1.4987866878509521,\n"," 'eval_precision': 0.4992406161857236,\n"," 'eval_recall': 0.567028092656481,\n"," 'eval_runtime': 566.0035,\n"," 'eval_samples_per_second': 54.116,\n"," 'eval_steps_per_second': 6.765}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_FNN.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"1js4hvMXti2A"},"source":["## 4.Fine-tuned on CoAID&FNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":50,"referenced_widgets":["3ddbe61ed9dd474db0daa13f53d679c0","c4e689a6e04b4d41874aaf764d777c2c","5be25a05dbc449199a335421b0e42a05","9dee0be6336f4a7f9243a6e4800e7828","d1b84f858a4047e88580b5a5f19697a6","fd334dddf690440ab54308c7629401d3","5cef2280c457458eb948b63724542609","4598a593ead34bf2803b8774f932ee6d","4496a47311324b249f440c6f99ae2897","b5abbdf7905944ca92276ed3d5cf9d24","c19c4c00c4dd49ecbb8f6aa5f4ffd1d2"]},"executionInfo":{"elapsed":5872,"status":"ok","timestamp":1655596517135,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"sqyUCF45nZAh","outputId":"2f608ff6-8acf-4584-82f4-5f8ad781a7b0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ddbe61ed9dd474db0daa13f53d679c0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":567659,"status":"ok","timestamp":1655597084786,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"mSygbfldtk1K","outputId":"b2429098-0f6d-49f3-8bb2-8da4d23ba614"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:22]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.7498857329415606,\n"," 'eval_f1': 0.7313909049472319,\n"," 'eval_loss': 0.9987914562225342,\n"," 'eval_precision': 0.8487265033770038,\n"," 'eval_recall': 0.6425579103006407,\n"," 'eval_runtime': 562.4497,\n"," 'eval_samples_per_second': 54.458,\n"," 'eval_steps_per_second': 6.808}"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_CoAID&FNN.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"fCAUCJvWtllA"},"source":["## 5.Fine-tuned on CoAID&PolitiFact"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":567516,"status":"ok","timestamp":1655597652285,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"GbP1Ipu0topw","outputId":"d64f3877-5804-43d7-a91e-557e4ffa8a23"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.8429970617042115,\n"," 'eval_f1': 0.8398654723452433,\n"," 'eval_loss': 0.7105494737625122,\n"," 'eval_precision': 0.9139068048409305,\n"," 'eval_recall': 0.7769221291276491,\n"," 'eval_runtime': 560.6595,\n"," 'eval_samples_per_second': 54.632,\n"," 'eval_steps_per_second': 6.829}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_CoAID&&PolitiFact_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"ov-NAlfetoPk"},"source":["## 6.Fine-tuned on CoAID & GossipCop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":566429,"status":"ok","timestamp":1655598218697,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"LUkSgBxXtxMU","outputId":"c1bc26af-c28a-45aa-e39d-46cda65d03ef"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:19]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.7403525954946131,\n"," 'eval_f1': 0.7214458337711463,\n"," 'eval_loss': 1.1903114318847656,\n"," 'eval_precision': 0.8360256514327462,\n"," 'eval_recall': 0.6344874322326269,\n"," 'eval_runtime': 559.6243,\n"," 'eval_samples_per_second': 54.733,\n"," 'eval_steps_per_second': 6.842}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_CoAID&GossipCop_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"3RPZBr4qtxty"},"source":["## 7.Fine-tuned on GossipCop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":565766,"status":"ok","timestamp":1655598784453,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"lbSPp2cftz4o","outputId":"590f6d05-beca-43e5-defc-801516357c78"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:19]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.3505060398302318,\n"," 'eval_f1': 0.5157961349364748,\n"," 'eval_loss': 1.1367509365081787,\n"," 'eval_precision': 0.42632976583246157,\n"," 'eval_recall': 0.6527846229669788,\n"," 'eval_runtime': 559.2367,\n"," 'eval_samples_per_second': 54.771,\n"," 'eval_steps_per_second': 6.847}"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_GossipCop_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"Pb4yA7UZt0gD"},"source":["## 8.Fine-tuned on PolitiFact"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":566784,"status":"ok","timestamp":1655599351217,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"0NpzxsWRt1h6","outputId":"0ccbb08d-b936-4cc0-8700-477d498a8859"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:19]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.7453476983349657,\n"," 'eval_f1': 0.7797978657331602,\n"," 'eval_loss': 1.0394972562789917,\n"," 'eval_precision': 0.7196977592496092,\n"," 'eval_recall': 0.8508501724987678,\n"," 'eval_runtime': 559.533,\n"," 'eval_samples_per_second': 54.742,\n"," 'eval_steps_per_second': 6.843}"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_PolitiFact_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"8dlR_yxJuMFe"},"source":["# Fake-News-Bert-Detect"]},{"cell_type":"markdown","metadata":{"id":"0ISNCPnlvLsl"},"source":["## 1. Out-of-box"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f3c671ea857d414cbef33cb2714540f9","7e39301292f64ae9a96e48713f8e96fa","02f4a8483585406b882932b6b7581c52","1367de35696244f8bb88f980b2566ba2","3587aaa0b37f4646bdb17c1b7b6bd4e4","441c82adfeaf4ccda8a81bc1956ccfdd","ca4faf641c6e41309ccb7de59ce8ffd3","057891106eb344168f9c43bfc15482f9","abae2f6e61974539b5b8c4fe6dca3fdb","70b82ef72dfc48d3aee2e87aca583204","bd50c05cc9b64c868e832fd86887109d","6a069bb622bd4991af62d577567be569","c55456a7b618462da542a45964328643","fbf709439dad475d9f0aa4c5236ffd29","b792c60b1b0f43c6b463703120bf70d6","bfbbf7c689124957a58dae5b41ec4f9b","b04c5010d6be4498af36df0c831b7c61","734515401cd14f488860c1bccb5c7144","97654dbb9ce3441ab15b069f7cba621a","8df868fa0d894ff882eaac96a1a9e637","c47ca839dc5d4e82a59451f223dfc53e","886dc163f2e1480991a6ddbfb5e5b970","73004da73cd54239b6d121ea57909345","b93757a370384bcbbb690194292a3fb4","6017cfcff4fa4da88dd0482d3978568d","2aa6cf732cb6453ea8468c8f19b12334","63594952a9d04fd1933b95a4a6b9dfa8","e8a61f5aed7f4405983b95b0aad316cc","a69d2146191240a3880368c039b1ef81","baca943f9d0049a5996316dbe8274f9a","5ad90dfc0ae74f2582a0fbb94d92f725","2f95e36bfc4341a8b73353ece17929b5","610b2f3de08446718c80b04aed1f00f7","4429712d1dc642f181938cdb432e61ef","11d9b157fc74484ea4df2df9289e400a","bdb5cf4a108941d6b383d4de1b89410c","9f437e65980d4dfa8239d39be957e8a0","e891a0014bd34e90a495847943b49e6a","0dd30cfba7024b53804d0fcc717eff0e","cf8f4c06d8b9475ba91861557fa419fa","5642c8bfbd224aa8bc3cfd5f7499852c","a07e57489b8d486b8feb0eb524ecd9e4","0820b79e68c4436d9183e580a72568c5","8ffa7d7118e64a529291d5c5cc368588","2e9b7a1a8ac64adc8332ee5a1c06bb72","1ebf41bc68da47a08c7e5b1c3bdc2390","1cc1514eb6654e4694cf8fb360f6e1a7","8aa6374aa1de460580c9307d45caa89c","b7c9df6c8aad4ed68768e2da4df7dc40","499ec30948404c068c2ee570577103ea","35ddb54bbafe43b082de3a936801e7d3","10f7b05cc5f44d188918d2198029828b","4d9759c046c949a1b0e67fda9f4eeb2e","f00a4170a26542fc8938a1f0c6ba6fe9","e96c861741d04295992e84ada72bc0d1","cba9f857b31f441fb1c515627a1c8a9b","48f6e65569f644c9a832a71735501b8a","7cb9fd3dadcd46ec977d5b9a3a58c0f1","567010dc3dc34d91bf4cd7a752761c09","86f2dbe9725c4274b2ed103b65aeb1b4","f6175ffbd35d4f34a57e7abdff62f1ad","fed9453482bc49f381638182f77f5a3e","e37a3ede985d4d2ea02baea88243c654","4d090c568c42419ca81061844bf5bf71","5a598ffb4d18484ba43a6c059ee3b128","1d4e7e29b8db4c7db28a42881a4f90f0"]},"executionInfo":{"elapsed":15318,"status":"ok","timestamp":1655599366520,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"0dN8rKDTvJZa","outputId":"0e77844a-f6fe-4b6a-a174-0d4f5409f994"},"outputs":[{"name":"stderr","output_type":"stream","text":["https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgk1glzob\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3c671ea857d414cbef33cb2714540f9","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/1bb953ac793c5ae15e9b5a3e1748d62cad5e9e5557408e989fb5d25f00ee6d0c.d6a300aa289d15142bf7dbad3b3a42b7c1ee1e0c874081ff0108003a2e307a2a\n","creating metadata file for /root/.cache/huggingface/transformers/1bb953ac793c5ae15e9b5a3e1748d62cad5e9e5557408e989fb5d25f00ee6d0c.d6a300aa289d15142bf7dbad3b3a42b7c1ee1e0c874081ff0108003a2e307a2a\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpks4p7k7j\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a069bb622bd4991af62d577567be569","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/5a3b85c1b5bf39e7feabb076838b178aa37c4e860c428b2fbd40ef1972f679a0.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","creating metadata file for /root/.cache/huggingface/transformers/5a3b85c1b5bf39e7feabb076838b178aa37c4e860c428b2fbd40ef1972f679a0.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpv2zfc1m4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73004da73cd54239b6d121ea57909345","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/abccee4c0ea0661b34063ece912c47e6d578f8ab52d66c02ea3387a3b5eb79b9.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","creating metadata file for /root/.cache/huggingface/transformers/abccee4c0ea0661b34063ece912c47e6d578f8ab52d66c02ea3387a3b5eb79b9.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3b7prnts\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4429712d1dc642f181938cdb432e61ef","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/6a4467084bef307312ff1896f1584b5a7f28fa88d2c2c39fa2e4905e6e2bdd2c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","creating metadata file for /root/.cache/huggingface/transformers/6a4467084bef307312ff1896f1584b5a7f28fa88d2c2c39fa2e4905e6e2bdd2c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/5a3b85c1b5bf39e7feabb076838b178aa37c4e860c428b2fbd40ef1972f679a0.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/abccee4c0ea0661b34063ece912c47e6d578f8ab52d66c02ea3387a3b5eb79b9.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/6a4467084bef307312ff1896f1584b5a7f28fa88d2c2c39fa2e4905e6e2bdd2c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/1bb953ac793c5ae15e9b5a3e1748d62cad5e9e5557408e989fb5d25f00ee6d0c.d6a300aa289d15142bf7dbad3b3a42b7c1ee1e0c874081ff0108003a2e307a2a\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpffdfjixt\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e9b7a1a8ac64adc8332ee5a1c06bb72","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/735 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","creating metadata file for /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppbc_den5\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cba9f857b31f441fb1c515627a1c8a9b","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","creating metadata file for /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]}],"source":["\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","def tokenize_data(example):\n","    return tokenizer(example['title'], padding='max_length')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400,"referenced_widgets":["46a6219531c4479394f14a9f01386f69","c96f260a7adf445f898394ae845ed3d9","522476062bbb4189952d7e250c960d13","760d6f89c4fe4d1e809c6d19da1a4458","826cc32c18b346b19afa46602939820e","ef6282e136674cfbbfdd444dd3390475","235b85006da24561bf45f61789a133ee","65b82cdba7344889b81b2902be173530","2d80e846f4b64b46a83c813b14cb9141","2bf0cd80b7114cbaba4501d87fd6e51b","d60de736f10641789969b8a8c26c6b89"]},"executionInfo":{"elapsed":567239,"status":"ok","timestamp":1655599933740,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"-DO6XRVfyXVu","outputId":"1bfa35ae-7c83-4142-dcbd-b93d2b73c489"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46a6219531c4479394f14a9f01386f69","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:20]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.4899771465883121,\n"," 'eval_f1': 0.15437912742232326,\n"," 'eval_loss': 2.6161410808563232,\n"," 'eval_precision': 0.6360392506690455,\n"," 'eval_recall': 0.08785115820601282,\n"," 'eval_runtime': 560.6432,\n"," 'eval_samples_per_second': 54.634,\n"," 'eval_steps_per_second': 6.83}"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["#Dataset\n","dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)\n","\n","#Eval Parameters\n","metric = load_metric(\"accuracy\")\n","test_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"L6boiGfAyE0U"},"source":["## 2.Finetuned on CoAID"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":567224,"status":"ok","timestamp":1655600500948,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"dUcvvhs9yGso","outputId":"db23ec07-6d03-490d-ce35-9c01dcae6c0c"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:19]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.9418543911198172,\n"," 'eval_f1': 0.9461396558502434,\n"," 'eval_loss': 0.36190667748451233,\n"," 'eval_precision': 0.9291951291951291,\n"," 'eval_recall': 0.9637136520453425,\n"," 'eval_runtime': 559.3718,\n"," 'eval_samples_per_second': 54.758,\n"," 'eval_steps_per_second': 6.845}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_shuffle_train_on_all_CoAID.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"xT1VujLhyHEF"},"source":["## 3.Fine-tuned on FNN "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":564911,"status":"ok","timestamp":1655601065851,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"3G1Fb7khyI1L","outputId":"49f4f2d0-d168-4bb8-fb0f-e62a5c5a0ba2"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:18]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.5299379693111329,\n"," 'eval_f1': 0.692757458068371,\n"," 'eval_loss': 0.6928958892822266,\n"," 'eval_precision': 0.5299379693111329,\n"," 'eval_recall': 1.0,\n"," 'eval_runtime': 558.3641,\n"," 'eval_samples_per_second': 54.857,\n"," 'eval_steps_per_second': 6.858}"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_Finetune_on_FNN.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"m1TyH4hVyJDz"},"source":["## 4.Fine-tuned on CoAID&FNN "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":565739,"status":"ok","timestamp":1655601631581,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"rW12njbVyK1M","outputId":"e8b09d34-0819-41d8-80ca-c1fc538b90a7"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:18]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.778256611165524,\n"," 'eval_f1': 0.7691366417403126,\n"," 'eval_loss': 1.1686644554138184,\n"," 'eval_precision': 0.8579011222323324,\n"," 'eval_recall': 0.6970182355840315,\n"," 'eval_runtime': 559.1448,\n"," 'eval_samples_per_second': 54.78,\n"," 'eval_steps_per_second': 6.848}"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_Finetune_on_CoAID&FNN.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"hv06SfKoyLRU"},"source":["## 5.Fine-tuned on CoAID&PolitiFact"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":564788,"status":"ok","timestamp":1655602196354,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"Oj8tH0zvyOHV","outputId":"f35e4ec3-ab7b-4a86-f4ca-343fadca64cf"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:17]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.9163238654913484,\n"," 'eval_f1': 0.9205985315530221,\n"," 'eval_loss': 0.46558281779289246,\n"," 'eval_precision': 0.9259051536112669,\n"," 'eval_recall': 0.915352390340069,\n"," 'eval_runtime': 558.0481,\n"," 'eval_samples_per_second': 54.888,\n"," 'eval_steps_per_second': 6.861}"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_CoAID&PolitiFact_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"tZbu3DamyOlD"},"source":["## 6.Fine-tuned on CoAID & GossipCop "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":564994,"status":"ok","timestamp":1655602761335,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"UV2mHGs6yQE3","outputId":"31bec10c-cec8-4eb0-fc9b-bc5b02032040"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:17]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.7271955599085863,\n"," 'eval_f1': 0.7157436385902842,\n"," 'eval_loss': 0.8935675621032715,\n"," 'eval_precision': 0.799149194773625,\n"," 'eval_recall': 0.6481025135534746,\n"," 'eval_runtime': 558.1648,\n"," 'eval_samples_per_second': 54.876,\n"," 'eval_steps_per_second': 6.86}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_CoAID&GossipCop_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"x8Mt9VxIyQYl"},"source":["## 7.Fine-tuned on GossipCop "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":565947,"status":"ok","timestamp":1655603327272,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"MxkImDRtySZW","outputId":"c885a6c2-4379-4171-e571-94234410e80a"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:17]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.37078028077048647,\n"," 'eval_f1': 0.5148150945296176,\n"," 'eval_loss': 1.2745856046676636,\n"," 'eval_precision': 0.4352730833085011,\n"," 'eval_recall': 0.6299285362247412,\n"," 'eval_runtime': 557.8817,\n"," 'eval_samples_per_second': 54.904,\n"," 'eval_steps_per_second': 6.863}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_GossipCop_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"j2Lp_veXySxA"},"source":["## 8.Fine-tuned on PolitiFact"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":567242,"status":"ok","timestamp":1655603894506,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"2KCMu-3DyT6c","outputId":"8ca8ec52-9a7a-4541-80d5-e6493411c212"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 30630\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3829' max='3829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3829/3829 09:18]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_accuracy': 0.7633692458374143,\n"," 'eval_f1': 0.7721041378442963,\n"," 'eval_loss': 1.1192243099212646,\n"," 'eval_precision': 0.7884664782943746,\n"," 'eval_recall': 0.7564070970921636,\n"," 'eval_runtime': 558.7716,\n"," 'eval_samples_per_second': 54.817,\n"," 'eval_steps_per_second': 6.853}"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_PolitiFact_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"xcM7DDPDvEjm"},"source":["#XLNet"]},{"cell_type":"markdown","metadata":{"id":"KcAm5KNxkXfd"},"source":["## Import XLNet tokenizer, used to convert our text into tokens that correspond to XLNet’s vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fa9mnfsg1x9A"},"outputs":[],"source":["import tensorflow as tf\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n","from transformers import AdamW\n","\n","from tqdm import tqdm, trange\n","import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from unicodedata import name\n","import pandas as pd\n","import glob, os\n","from datasets import Dataset\n","import numpy as np\n","from datasets import load_metric\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":28971,"status":"ok","timestamp":1655603923626,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"gwe1mJX9k_Nc","outputId":"2d2baa1a-fcb3-47db-f088-cce50d62eb85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 7.5 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting simpletransformers\n","  Downloading simpletransformers-0.63.7-py3-none-any.whl (249 kB)\n","\u001b[K     |████████████████████████████████| 249 kB 8.7 MB/s \n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.1.96)\n","Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.20.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.4.1)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.8.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.3.2)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.12.1)\n","Collecting wandb>=0.10.32\n","  Downloading wandb-0.12.18-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 53.9 MB/s \n","\u001b[?25hCollecting streamlit\n","  Downloading streamlit-1.10.0-py2.py3-none-any.whl (9.1 MB)\n","\u001b[K     |████████████████████████████████| 9.1 MB 49.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.64.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.3.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2022.6.2)\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (3.7.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (4.11.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (0.7.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.6.0->simpletransformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.9)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.15.0)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.17.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (5.4.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (57.4.0)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 56.5 MB/s \n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (7.1.2)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 70.9 MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting setproctitle\n","  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.18.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.8.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.5.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.0.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2022.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.70.13)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (6.0.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (21.4.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (2.0.12)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.7.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.6.0->simpletransformers) (3.8.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2022.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (1.1.0)\n","Collecting pydeck>=0.1.dev5\n","  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 57.5 MB/s \n","\u001b[?25hCollecting validators\n","  Downloading validators-0.20.0.tar.gz (30 kB)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.4)\n","Collecting pympler>=0.9\n","  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n","\u001b[K     |████████████████████████████████| 164 kB 68.9 MB/s \n","\u001b[?25hCollecting blinker\n","  Downloading blinker-1.4.tar.gz (111 kB)\n","\u001b[K     |████████████████████████████████| 111 kB 77.4 MB/s \n","\u001b[?25hCollecting watchdog\n","  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n","Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.13.0)\n","Collecting toml\n","  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.0)\n","Collecting rich\n","  Downloading rich-12.4.4-py3-none-any.whl (232 kB)\n","\u001b[K     |████████████████████████████████| 232 kB 74.5 MB/s \n","\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (5.1.1)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.2)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (5.7.1)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.1)\n","Collecting ipykernel>=5.1.2\n","  Downloading ipykernel-6.15.0-py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 72.5 MB/s \n","\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.7.0)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.1)\n","Collecting tornado>=5.0\n","  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n","\u001b[K     |████████████████████████████████| 428 kB 59.7 MB/s \n","\u001b[?25hCollecting ipython>=7.23.1\n","  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 63.0 MB/s \n","\u001b[?25hCollecting jupyter-client>=6.1.12\n","  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (23.1.0)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.5)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.3)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.18.1)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n","\u001b[K     |████████████████████████████████| 381 kB 74.3 MB/s \n","\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.6.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.4.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.1.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n","Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.10.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.15.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.13.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.8.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.46.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.37.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (3.3.7)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.35.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.0)\n","Building wheels for collected packages: pathtools, seqeval, blinker, validators\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=dd501dc8ca7b9a42932c9cb6578b0bba1df7049dab7150057f9038df15e70533\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=a5e011945653d31d4c5178760a1bc513d4f8a480c4bb4af1aa04aa0c4f237a31\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=68a661bbd3fe6ee0f10edfb10c4e368bbdc607748cb7c773e75a59fa539750ab\n","  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n","  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=1f26246ee54bed93ce6f9fb243199ec00b42718626cee393b8a1115158737007\n","  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n","Successfully built pathtools seqeval blinker validators\n","Installing collected packages: tornado, prompt-toolkit, jupyter-client, ipython, ipykernel, smmap, gitdb, commonmark, watchdog, validators, toml, shortuuid, setproctitle, sentry-sdk, rich, pympler, pydeck, pathtools, GitPython, docker-pycreds, blinker, wandb, streamlit, seqeval, simpletransformers\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 5.1.1\n","    Uninstalling tornado-5.1.1:\n","      Successfully uninstalled tornado-5.1.1\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: jupyter-client\n","    Found existing installation: jupyter-client 5.3.5\n","    Uninstalling jupyter-client-5.3.5:\n","      Successfully uninstalled jupyter-client-5.3.5\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","nbclient 0.6.4 requires traitlets>=5.2.2, but you have traitlets 5.1.1 which is incompatible.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n","google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.15.0 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n","google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n","Successfully installed GitPython-3.1.27 blinker-1.4 commonmark-0.9.1 docker-pycreds-0.4.0 gitdb-4.0.9 ipykernel-6.15.0 ipython-7.34.0 jupyter-client-7.3.4 pathtools-0.1.2 prompt-toolkit-3.0.29 pydeck-0.7.1 pympler-1.0.1 rich-12.4.4 sentry-sdk-1.5.12 seqeval-1.2.2 setproctitle-1.2.3 shortuuid-1.0.9 simpletransformers-0.63.7 smmap-5.0.0 streamlit-1.10.0 toml-0.10.2 tornado-6.1 validators-0.20.0 wandb-0.12.18 watchdog-2.1.9\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","prompt_toolkit","tornado"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install sentencepiece \n","!pip install simpletransformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":82,"referenced_widgets":["c348a1b176e64f07aeb35911322fc26b","5dd6e17fbc264b6d87a2bba9d059d3cb","62cc8b6aaefc49daac9de5152a3c322d","d172141a8e894262baf7180443081f13","5d34d47d21534c9b91bcab9cf95dda4a","a49a5245c03445a19d4c94df8a232f78","e28646ec11c04c4b81b627a9caa5badf","da8ad50ad8de4f1b83413937c024634f","853473755a04474293aa51ee9dcf5dd6","1badb2a2646c4ea785259b8ce3355d6c","93b1034edd93482cb08ec43fb303a80b","d2ef824f280f4aa7ae140bd4960183f4","ef98240540a84037a68f80d259d4211c","408abacc72a44a97b8564067fcf96143","4715a0f6d0ad4ee480a07f059f82adf2","6dd874080a334f10bfae16878c23ba77","4090644364594668a5a1e65ea4d4b3bd","6be533055a5146b090dee076c13e145f","517bdc1274af481f948979829e9a088e","a4d2019c67074c1aa86103bbc39dc253","475d15be90524a91b2e0b3729b6a9692","f3250b800fa849bfac9a2904c43b962e"]},"executionInfo":{"elapsed":2288,"status":"ok","timestamp":1655604070506,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"-DzOWMHSkVcZ","outputId":"0ea5414e-b0e4-4845-9e77-f268530b9d19"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c348a1b176e64f07aeb35911322fc26b","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2ef824f280f4aa7ae140bd4960183f4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n"]},{"cell_type":"markdown","metadata":{"id":"eohD9aF1vQG3"},"source":["## 1. Out-of-box "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180,"referenced_widgets":["940fd339d91c40b1a9e32c996e3a6e0a","0a714508d38841619b4b7bc7ded6e73d","20e63b536e3c4f588f65641c3eb2fcad","7b244b72210d40afb78a0734b608a579","cd30630207d9437d8e1c04b7b06ef4db","8084e16f36404c9dbe110bd43e1a0d28","066ce0ba18334df99611d4961863e0a8","f319a1047b3c45ab9b6ad98857bcb0fd","e457aa356e3d40d7a9e364d98fed204e","a4ce5c2b5e0d4207a1c34eeaecd28db9","b19d9bc5b1f4485e81db507e4c7cf83d"]},"executionInfo":{"elapsed":15065,"status":"ok","timestamp":1655604085569,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"-6xlX91BvEOl","outputId":"7b748ed9-dad6-48d9-e384-d2d4befba5a3"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"940fd339d91c40b1a9e32c996e3a6e0a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80mzkX6J2Tln"},"outputs":[],"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hy7OfywR1_dx"},"outputs":[],"source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NN1RUv62bPF"},"outputs":[],"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6XjBmIk3NXt"},"outputs":[],"source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160,"status":"ok","timestamp":1655604486025,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"_mEZ5IFi3Nvv","outputId":"96789b59-07d2-4594-f853-1df553127a3c"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.5300032647730982,\n"," 'f1': 0.6927870251813915,\n"," 'precision': 0.5299725741151887,\n"," 'recall': 1.0}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","{\n","        'accuracy': accuracy_score(flat_true_labels, flat_predictions),\n","        'f1': f1_score(flat_true_labels, flat_predictions),\n","        'precision': precision_score(flat_true_labels, flat_predictions),\n","        'recall': recall_score(flat_true_labels, flat_predictions)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"exHofZHWzMX3"},"source":["## 2.Finetuned on CoAID "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10061,"status":"ok","timestamp":1655604506677,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"FzekEhhbzMBW","outputId":"ff39cc0f-53d4-479a-c273-969ac0f9b0e9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_CoAID.pt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPepdGID3iYw"},"outputs":[],"source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nWALeBO3i9I"},"outputs":[],"source":["# Prediction on test set\n","model.cuda()\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XoIbtki3lZ6"},"outputs":[],"source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655604706180,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"C8OOxgqF3l9l","outputId":"f095cb9b-abe3-432c-8817-616b88e6be33"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.9116552399608228,\n"," 'f1': 0.9223440280089535,\n"," 'precision': 0.8633286773396368,\n"," 'recall': 0.990019714144899}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","{\n","        'accuracy': accuracy_score(flat_true_labels, flat_predictions),\n","        'f1': f1_score(flat_true_labels, flat_predictions),\n","        'precision': precision_score(flat_true_labels, flat_predictions),\n","        'recall': recall_score(flat_true_labels, flat_predictions)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"8BDojqSVzOPC"},"source":["## 3.Fine-tuned on FNN "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9234,"status":"ok","timestamp":1655604715408,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"8rfZILW6zQoB","outputId":"97b49075-7ab8-46a3-890a-b0d8beb76cff"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_FNN.pt'))\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q72CfOt63q8I"},"outputs":[],"source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgh94bkA3r_A"},"outputs":[],"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xL9dx4Jb3tFp"},"outputs":[],"source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92,"status":"ok","timestamp":1655604893544,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"rnRm0MFM3uQm","outputId":"9f6c8f91-9d09-4a7d-c9cf-68af85c052a9"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.6650669278485145,\n"," 'f1': 0.7489170072689003,\n"," 'precision': 0.6212693385308807,\n"," 'recall': 0.9425825529817644}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","{\n","        'accuracy': accuracy_score(flat_true_labels, flat_predictions),\n","        'f1': f1_score(flat_true_labels, flat_predictions),\n","        'precision': precision_score(flat_true_labels, flat_predictions),\n","        'recall': recall_score(flat_true_labels, flat_predictions)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"ddhqs6nrzQIf"},"source":["## 4.Fine-tuned on CoAID&FNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11964,"status":"ok","timestamp":1655604905501,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"0N_s5b0OzP9l","outputId":"33f35634-b757-4311-eaf9-ff4d6565ecfb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_CoAID_FNN.pt'))\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ot07ma9A3x2k"},"outputs":[],"source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGrOFzMu972k"},"outputs":[],"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0TvY_Hw31uO"},"outputs":[],"source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655605081407,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"BUhOwEqJ37Lq","outputId":"df922475-4fbd-4222-95db-09caefdbee97"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.9206333659810643,\n"," 'f1': 0.9259631490787269,\n"," 'precision': 0.9156176594591339,\n"," 'recall': 0.9365450961064564}"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","{\n","        'accuracy': accuracy_score(flat_true_labels, flat_predictions),\n","        'f1': f1_score(flat_true_labels, flat_predictions),\n","        'precision': precision_score(flat_true_labels, flat_predictions),\n","        'recall': recall_score(flat_true_labels, flat_predictions)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"uoSxQQIuzSjF"},"source":["## 5.Fine-tuned on CoAID&PolitiFact "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10574,"status":"ok","timestamp":1655605091975,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"WbWoLH9XzU_c","outputId":"88cdf60f-cf4f-46f8-a218-aa91a2d53c15"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_CoAID_Politi.pt'))\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqeCyyG43yV3"},"outputs":[],"source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbu9xyyI9991"},"outputs":[],"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qx6d5Vb432KK"},"outputs":[],"source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655605267362,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"dIkxS6De37tY","outputId":"a4c5af78-f60f-4d11-a7fd-755ace3b8b8f"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8985308521057787,\n"," 'f1': 0.9011953204476094,\n"," 'precision': 0.9310299527062533,\n"," 'recall': 0.8732134056185313}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","{\n","        'accuracy': accuracy_score(flat_true_labels, flat_predictions),\n","        'f1': f1_score(flat_true_labels, flat_predictions),\n","        'precision': precision_score(flat_true_labels, flat_predictions),\n","        'recall': recall_score(flat_true_labels, flat_predictions)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"a-MAXazxzVN5"},"source":["## 6.Fine-tuned on CoAID & GossipCop "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9728,"status":"ok","timestamp":1655605277084,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"rn-nGQKxzXNR","outputId":"27486251-de26-468a-bf3c-d5f2fc493610"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_CoAID_Gossip.pt'))\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxUj5C2B3y7z"},"outputs":[],"source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9YUnXwJ9_Mf"},"outputs":[],"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCqNjj5A32kN"},"outputs":[],"source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655605452637,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"jiAWomwo38MD","outputId":"03075c51-3e64-4af2-c5ca-325aab0c939f"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8668625530525629,\n"," 'f1': 0.8776477647764777,\n"," 'precision': 0.8554216867469879,\n"," 'recall': 0.9010596352883193}"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","{\n","        'accuracy': accuracy_score(flat_true_labels, flat_predictions),\n","        'f1': f1_score(flat_true_labels, flat_predictions),\n","        'precision': precision_score(flat_true_labels, flat_predictions),\n","        'recall': recall_score(flat_true_labels, flat_predictions)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"tlxhbQGGzXnY"},"source":["## 7.Fine-tuned on GossipCop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11065,"status":"ok","timestamp":1655605463696,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"uHkHb7ZFzZVU","outputId":"7f9f426d-ef9b-47ea-fae6-714515b4553c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_Gossip.pt'))\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKI1Cpxq3zeu"},"outputs":[],"source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8oNNpq9p-AHd"},"outputs":[],"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0XK60Ip33Cp"},"outputs":[],"source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1655605639781,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"vSskyiIW38rS","outputId":"5cff3d0a-2ebd-4b0f-8ecf-cff03888e061"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.493111328762651,\n"," 'f1': 0.5680743337228064,\n"," 'precision': 0.517906056609516,\n"," 'recall': 0.6290044356826022}"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","{\n","        'accuracy': accuracy_score(flat_true_labels, flat_predictions),\n","        'f1': f1_score(flat_true_labels, flat_predictions),\n","        'precision': precision_score(flat_true_labels, flat_predictions),\n","        'recall': recall_score(flat_true_labels, flat_predictions)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"V6YLZBBWzZk_"},"source":["## 8.Fine-tuned on PolitiFact"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10110,"status":"ok","timestamp":1655605649880,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"Ao8WW1qcza3p","outputId":"557d687a-0138-4c40-fb04-c909c1639634"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_Politi.pt'))\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OwEWgvp3z-Z"},"outputs":[],"source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWvfjq3T-BI9"},"outputs":[],"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZB_lLJW33wM"},"outputs":[],"source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655605825212,"user":{"displayName":"yunong liu","userId":"01039174376626853323"},"user_tz":300},"id":"nnZNuHWS39JD","outputId":"c4717a58-5a1a-4d47-9ab4-dfc34071acdf"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.7942213516160627,\n"," 'f1': 0.81664,\n"," 'precision': 0.7736317036873726,\n"," 'recall': 0.8647116806308527}"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","{\n","        'accuracy': accuracy_score(flat_true_labels, flat_predictions),\n","        'f1': f1_score(flat_true_labels, flat_predictions),\n","        'precision': precision_score(flat_true_labels, flat_predictions),\n","        'recall': recall_score(flat_true_labels, flat_predictions)\n","    }"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPL07om8aegNdi2iz+F7XFH","collapsed_sections":["MwpKjHAB8az7","aAA6N_G_94eT","m7Zv7S-4_ie9","RHT4xiF99-Vb","jpzFJI7Ynb0j","-meGXc9Y-0cF","MqWbBiOm-8oF","rE_pP9jp_Eeh","mn3agbV__R6T","Wv7BP-oL_YTz","YjCDKt2itCT_","d7WvZAuNtVsU","IAFx2my9te5b","1js4hvMXti2A","fCAUCJvWtllA","ov-NAlfetoPk","3RPZBr4qtxty","Pb4yA7UZt0gD","8dlR_yxJuMFe","0ISNCPnlvLsl","L6boiGfAyE0U","xT1VujLhyHEF","m1TyH4hVyJDz","hv06SfKoyLRU","tZbu3DamyOlD","x8Mt9VxIyQYl","j2Lp_veXySxA","-zELZJxhvGdQ","Eh0gGG_tvRe5"],"mount_file_id":"1wyh0KWu1e3MomRSboWXj2BXLVYJsPuYr","name":"Test_on_New_Datatset.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"bf3dd0236868da6ddc6b668912a1878ecf12eb960791ea0549fa4b3c069e6350"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"02f4a8483585406b882932b6b7581c52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_057891106eb344168f9c43bfc15482f9","max":1140,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abae2f6e61974539b5b8c4fe6dca3fdb","value":1140}},"03a6d5047d99473ea60cedc61508ef1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0500ae8f031b4bde9833e40e94916336":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"057891106eb344168f9c43bfc15482f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06290a60d2814d53af4c26d85d14b193":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"066ce0ba18334df99611d4961863e0a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0820b79e68c4436d9183e580a72568c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a714508d38841619b4b7bc7ded6e73d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8084e16f36404c9dbe110bd43e1a0d28","placeholder":"​","style":"IPY_MODEL_066ce0ba18334df99611d4961863e0a8","value":"Downloading: 100%"}},"0c08d5610f2b4ee99cc7c8827598e958":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db8edd12161b425ea45a877c9bcfbbcd","placeholder":"​","style":"IPY_MODEL_4b4be4f8ca7f4a0287b2683dafad38ff","value":"100%"}},"0c6e5d752fa8429c9080dc9334abbe01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dd30cfba7024b53804d0fcc717eff0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10d5aa36dfd3443787a7aeee5161b3c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10f7b05cc5f44d188918d2198029828b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11d9b157fc74484ea4df2df9289e400a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dd30cfba7024b53804d0fcc717eff0e","placeholder":"​","style":"IPY_MODEL_cf8f4c06d8b9475ba91861557fa419fa","value":"Downloading: 100%"}},"11f96e5bffed448385841900683d95fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12d8f144439649f89e70e78e2f247bff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1367de35696244f8bb88f980b2566ba2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70b82ef72dfc48d3aee2e87aca583204","placeholder":"​","style":"IPY_MODEL_bd50c05cc9b64c868e832fd86887109d","value":" 1.11k/1.11k [00:00&lt;00:00, 31.1kB/s]"}},"149dc4b93bf849e5bf09d33066a729fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06290a60d2814d53af4c26d85d14b193","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7914b8eb8fa347148c2282f856bbd6b0","value":31}},"1badb2a2646c4ea785259b8ce3355d6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cc1514eb6654e4694cf8fb360f6e1a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10f7b05cc5f44d188918d2198029828b","max":735,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d9759c046c949a1b0e67fda9f4eeb2e","value":735}},"1d4e7e29b8db4c7db28a42881a4f90f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ebf41bc68da47a08c7e5b1c3bdc2390":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_499ec30948404c068c2ee570577103ea","placeholder":"​","style":"IPY_MODEL_35ddb54bbafe43b082de3a936801e7d3","value":"Downloading: 100%"}},"20e63b536e3c4f588f65641c3eb2fcad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f319a1047b3c45ab9b6ad98857bcb0fd","max":467042463,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e457aa356e3d40d7a9e364d98fed204e","value":467042463}},"235b85006da24561bf45f61789a133ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2460aba7842c44058707a3c87250ba40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28955720ae3f4c89b4ae09c393b24c10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b76dcf719534665a75d375be2428405","IPY_MODEL_7a5e4a6f44c54a8bb249030be6380ec5","IPY_MODEL_80cab5c8fd9e4e67b51ec61b542deba9"],"layout":"IPY_MODEL_11f96e5bffed448385841900683d95fe"}},"2aa6cf732cb6453ea8468c8f19b12334":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f95e36bfc4341a8b73353ece17929b5","placeholder":"​","style":"IPY_MODEL_610b2f3de08446718c80b04aed1f00f7","value":" 446k/446k [00:00&lt;00:00, 717kB/s]"}},"2bf0cd80b7114cbaba4501d87fd6e51b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d80e846f4b64b46a83c813b14cb9141":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e9b7a1a8ac64adc8332ee5a1c06bb72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ebf41bc68da47a08c7e5b1c3bdc2390","IPY_MODEL_1cc1514eb6654e4694cf8fb360f6e1a7","IPY_MODEL_8aa6374aa1de460580c9307d45caa89c"],"layout":"IPY_MODEL_b7c9df6c8aad4ed68768e2da4df7dc40"}},"2f95e36bfc4341a8b73353ece17929b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30ac56e6f1fa4312a0f625f0452e0b91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3587aaa0b37f4646bdb17c1b7b6bd4e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35ddb54bbafe43b082de3a936801e7d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bb2ab038c1144e3910979483e20d945":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ddbe61ed9dd474db0daa13f53d679c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4e689a6e04b4d41874aaf764d777c2c","IPY_MODEL_5be25a05dbc449199a335421b0e42a05","IPY_MODEL_9dee0be6336f4a7f9243a6e4800e7828"],"layout":"IPY_MODEL_d1b84f858a4047e88580b5a5f19697a6"}},"3f230d91a1b8467eabca1f3137778647":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa54875fdb34b568f0539ada8e4715c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"408abacc72a44a97b8564067fcf96143":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_517bdc1274af481f948979829e9a088e","max":760,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4d2019c67074c1aa86103bbc39dc253","value":760}},"4090644364594668a5a1e65ea4d4b3bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"441c82adfeaf4ccda8a81bc1956ccfdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4429712d1dc642f181938cdb432e61ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11d9b157fc74484ea4df2df9289e400a","IPY_MODEL_bdb5cf4a108941d6b383d4de1b89410c","IPY_MODEL_9f437e65980d4dfa8239d39be957e8a0"],"layout":"IPY_MODEL_e891a0014bd34e90a495847943b49e6a"}},"4496a47311324b249f440c6f99ae2897":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4598a593ead34bf2803b8774f932ee6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45f163089f9b42e488722f11899b00e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03a6d5047d99473ea60cedc61508ef1d","placeholder":"​","style":"IPY_MODEL_12d8f144439649f89e70e78e2f247bff","value":"100%"}},"46a6219531c4479394f14a9f01386f69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c96f260a7adf445f898394ae845ed3d9","IPY_MODEL_522476062bbb4189952d7e250c960d13","IPY_MODEL_760d6f89c4fe4d1e809c6d19da1a4458"],"layout":"IPY_MODEL_826cc32c18b346b19afa46602939820e"}},"4715a0f6d0ad4ee480a07f059f82adf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_475d15be90524a91b2e0b3729b6a9692","placeholder":"​","style":"IPY_MODEL_f3250b800fa849bfac9a2904c43b962e","value":" 760/760 [00:00&lt;00:00, 23.5kB/s]"}},"475d15be90524a91b2e0b3729b6a9692":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"477c1884e49d4afba83ac68fc9952bb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45f163089f9b42e488722f11899b00e6","IPY_MODEL_ca03003c9d51469cbb3cebd7b1f713a9","IPY_MODEL_7ec597fdcb5c40cc925b66d2ac54901a"],"layout":"IPY_MODEL_3bb2ab038c1144e3910979483e20d945"}},"48f6e65569f644c9a832a71735501b8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6175ffbd35d4f34a57e7abdff62f1ad","placeholder":"​","style":"IPY_MODEL_fed9453482bc49f381638182f77f5a3e","value":"Downloading: 100%"}},"499ec30948404c068c2ee570577103ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b4be4f8ca7f4a0287b2683dafad38ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d090c568c42419ca81061844bf5bf71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d9759c046c949a1b0e67fda9f4eeb2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"517bdc1274af481f948979829e9a088e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"522476062bbb4189952d7e250c960d13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65b82cdba7344889b81b2902be173530","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d80e846f4b64b46a83c813b14cb9141","value":31}},"5642c8bfbd224aa8bc3cfd5f7499852c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"567010dc3dc34d91bf4cd7a752761c09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a598ffb4d18484ba43a6c059ee3b128","placeholder":"​","style":"IPY_MODEL_1d4e7e29b8db4c7db28a42881a4f90f0","value":" 476M/476M [00:08&lt;00:00, 58.6MB/s]"}},"59a7c632cf55450996f0e2a7921404c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d578f2c9e4e64ab4b2c383889fcab4ea","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fa54875fdb34b568f0539ada8e4715c","value":31}},"5a598ffb4d18484ba43a6c059ee3b128":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ad90dfc0ae74f2582a0fbb94d92f725":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b3584c558254b898013e72e88ffd776":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5be25a05dbc449199a335421b0e42a05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4598a593ead34bf2803b8774f932ee6d","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4496a47311324b249f440c6f99ae2897","value":31}},"5cef2280c457458eb948b63724542609":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d34d47d21534c9b91bcab9cf95dda4a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dd6e17fbc264b6d87a2bba9d059d3cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a49a5245c03445a19d4c94df8a232f78","placeholder":"​","style":"IPY_MODEL_e28646ec11c04c4b81b627a9caa5badf","value":"Downloading: 100%"}},"5f0b0a259a0a4923ba92003237c85b65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6017cfcff4fa4da88dd0482d3978568d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_baca943f9d0049a5996316dbe8274f9a","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ad90dfc0ae74f2582a0fbb94d92f725","value":456318}},"60bf5603a1614e05b47e035a35cb3e3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"610b2f3de08446718c80b04aed1f00f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61c6adbff1fb47de87fe03187b00537c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0500ae8f031b4bde9833e40e94916336","placeholder":"​","style":"IPY_MODEL_b84463644c7d417d98002c8e03b8de80","value":" 31/31 [00:05&lt;00:00,  5.62ba/s]"}},"62cc8b6aaefc49daac9de5152a3c322d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da8ad50ad8de4f1b83413937c024634f","max":798011,"min":0,"orientation":"horizontal","style":"IPY_MODEL_853473755a04474293aa51ee9dcf5dd6","value":798011}},"63594952a9d04fd1933b95a4a6b9dfa8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6592309d4696478b9cf013628808b9c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"659a998199d84fdf929731b704ae79d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65b82cdba7344889b81b2902be173530":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a069bb622bd4991af62d577567be569":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c55456a7b618462da542a45964328643","IPY_MODEL_fbf709439dad475d9f0aa4c5236ffd29","IPY_MODEL_b792c60b1b0f43c6b463703120bf70d6"],"layout":"IPY_MODEL_bfbbf7c689124957a58dae5b41ec4f9b"}},"6be533055a5146b090dee076c13e145f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6dd874080a334f10bfae16878c23ba77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70b82ef72dfc48d3aee2e87aca583204":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73004da73cd54239b6d121ea57909345":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b93757a370384bcbbb690194292a3fb4","IPY_MODEL_6017cfcff4fa4da88dd0482d3978568d","IPY_MODEL_2aa6cf732cb6453ea8468c8f19b12334"],"layout":"IPY_MODEL_63594952a9d04fd1933b95a4a6b9dfa8"}},"734515401cd14f488860c1bccb5c7144":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"760d6f89c4fe4d1e809c6d19da1a4458":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bf0cd80b7114cbaba4501d87fd6e51b","placeholder":"​","style":"IPY_MODEL_d60de736f10641789969b8a8c26c6b89","value":" 31/31 [00:05&lt;00:00,  5.75ba/s]"}},"784d6a63273a49558e0af472da52e53a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7914b8eb8fa347148c2282f856bbd6b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a5e4a6f44c54a8bb249030be6380ec5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c6e5d752fa8429c9080dc9334abbe01","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_30ac56e6f1fa4312a0f625f0452e0b91","value":31}},"7b244b72210d40afb78a0734b608a579":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4ce5c2b5e0d4207a1c34eeaecd28db9","placeholder":"​","style":"IPY_MODEL_b19d9bc5b1f4485e81db507e4c7cf83d","value":" 445M/445M [00:08&lt;00:00, 60.2MB/s]"}},"7c6acfcc13c346a1a0ae364c6d55bb3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60bf5603a1614e05b47e035a35cb3e3d","placeholder":"​","style":"IPY_MODEL_f99a96b6824347c78a0b661df7e52288","value":"100%"}},"7cb9fd3dadcd46ec977d5b9a3a58c0f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e37a3ede985d4d2ea02baea88243c654","max":498657517,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d090c568c42419ca81061844bf5bf71","value":498657517}},"7e39301292f64ae9a96e48713f8e96fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_441c82adfeaf4ccda8a81bc1956ccfdd","placeholder":"​","style":"IPY_MODEL_ca4faf641c6e41309ccb7de59ce8ffd3","value":"Downloading: 100%"}},"7ec597fdcb5c40cc925b66d2ac54901a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca026651ce06469c8f757f5a8c4700de","placeholder":"​","style":"IPY_MODEL_6592309d4696478b9cf013628808b9c6","value":" 31/31 [00:13&lt;00:00,  1.91ba/s]"}},"8084e16f36404c9dbe110bd43e1a0d28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80cab5c8fd9e4e67b51ec61b542deba9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_784d6a63273a49558e0af472da52e53a","placeholder":"​","style":"IPY_MODEL_f040058a407a48dd87871fe1dc4c0c30","value":" 31/31 [00:07&lt;00:00,  4.69ba/s]"}},"826cc32c18b346b19afa46602939820e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"843627041fc14ee2b647b4eabc4488e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8938483084aa4ca7a405b61995c0ccb8","placeholder":"​","style":"IPY_MODEL_c080e8bd441e4b3e9e626e0666b451e1","value":" 31/31 [00:05&lt;00:00,  5.77ba/s]"}},"853473755a04474293aa51ee9dcf5dd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86595ccf9a254d61b67f01cdbe26e73f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6ddd167d85c4c48bb112081a86a1639","IPY_MODEL_97485028605e458aada7c0663b3ccea1","IPY_MODEL_843627041fc14ee2b647b4eabc4488e7"],"layout":"IPY_MODEL_2460aba7842c44058707a3c87250ba40"}},"86f2dbe9725c4274b2ed103b65aeb1b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"886dc163f2e1480991a6ddbfb5e5b970":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8938483084aa4ca7a405b61995c0ccb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"897e7b88bc6d4f6e82158742a3f9cb68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8aa6374aa1de460580c9307d45caa89c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f00a4170a26542fc8938a1f0c6ba6fe9","placeholder":"​","style":"IPY_MODEL_e96c861741d04295992e84ada72bc0d1","value":" 735/735 [00:00&lt;00:00, 23.3kB/s]"}},"8b76dcf719534665a75d375be2428405":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba6c763e1f3148acbe647564fb632dcd","placeholder":"​","style":"IPY_MODEL_897e7b88bc6d4f6e82158742a3f9cb68","value":"100%"}},"8df868fa0d894ff882eaac96a1a9e637":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ffa7d7118e64a529291d5c5cc368588":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93b1034edd93482cb08ec43fb303a80b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"940fd339d91c40b1a9e32c996e3a6e0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a714508d38841619b4b7bc7ded6e73d","IPY_MODEL_20e63b536e3c4f588f65641c3eb2fcad","IPY_MODEL_7b244b72210d40afb78a0734b608a579"],"layout":"IPY_MODEL_cd30630207d9437d8e1c04b7b06ef4db"}},"95e5a5e414f542cfa0de676bc9369e28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97485028605e458aada7c0663b3ccea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f0b0a259a0a4923ba92003237c85b65","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bfad58c8124d4f8c95c98c4503871c07","value":31}},"97654dbb9ce3441ab15b069f7cba621a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bd74301d6e943f49ebc7f8751bec137":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c08d5610f2b4ee99cc7c8827598e958","IPY_MODEL_149dc4b93bf849e5bf09d33066a729fc","IPY_MODEL_61c6adbff1fb47de87fe03187b00537c"],"layout":"IPY_MODEL_cd52726c54c54878b01eebc00523401a"}},"9dee0be6336f4a7f9243a6e4800e7828":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5abbdf7905944ca92276ed3d5cf9d24","placeholder":"​","style":"IPY_MODEL_c19c4c00c4dd49ecbb8f6aa5f4ffd1d2","value":" 31/31 [00:05&lt;00:00,  5.54ba/s]"}},"9f437e65980d4dfa8239d39be957e8a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0820b79e68c4436d9183e580a72568c5","placeholder":"​","style":"IPY_MODEL_8ffa7d7118e64a529291d5c5cc368588","value":" 772/772 [00:00&lt;00:00, 22.1kB/s]"}},"a07e57489b8d486b8feb0eb524ecd9e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a49a5245c03445a19d4c94df8a232f78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4ce5c2b5e0d4207a1c34eeaecd28db9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4d2019c67074c1aa86103bbc39dc253":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a69d2146191240a3880368c039b1ef81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abae2f6e61974539b5b8c4fe6dca3fdb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b04c5010d6be4498af36df0c831b7c61":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b19d9bc5b1f4485e81db507e4c7cf83d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5abbdf7905944ca92276ed3d5cf9d24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b792c60b1b0f43c6b463703120bf70d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c47ca839dc5d4e82a59451f223dfc53e","placeholder":"​","style":"IPY_MODEL_886dc163f2e1480991a6ddbfb5e5b970","value":" 878k/878k [00:00&lt;00:00, 920kB/s]"}},"b7c9df6c8aad4ed68768e2da4df7dc40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b84463644c7d417d98002c8e03b8de80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b93757a370384bcbbb690194292a3fb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8a61f5aed7f4405983b95b0aad316cc","placeholder":"​","style":"IPY_MODEL_a69d2146191240a3880368c039b1ef81","value":"Downloading: 100%"}},"ba6c763e1f3148acbe647564fb632dcd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"baca943f9d0049a5996316dbe8274f9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd50c05cc9b64c868e832fd86887109d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdb5cf4a108941d6b383d4de1b89410c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5642c8bfbd224aa8bc3cfd5f7499852c","max":772,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a07e57489b8d486b8feb0eb524ecd9e4","value":772}},"bfad58c8124d4f8c95c98c4503871c07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bfbbf7c689124957a58dae5b41ec4f9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c080e8bd441e4b3e9e626e0666b451e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c16ef68e7521495abf00badf2a99d2d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f230d91a1b8467eabca1f3137778647","placeholder":"​","style":"IPY_MODEL_db104488ce674ca6a1697f0bb7461e19","value":" 31/31 [00:05&lt;00:00,  5.75ba/s]"}},"c19c4c00c4dd49ecbb8f6aa5f4ffd1d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c348a1b176e64f07aeb35911322fc26b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5dd6e17fbc264b6d87a2bba9d059d3cb","IPY_MODEL_62cc8b6aaefc49daac9de5152a3c322d","IPY_MODEL_d172141a8e894262baf7180443081f13"],"layout":"IPY_MODEL_5d34d47d21534c9b91bcab9cf95dda4a"}},"c47ca839dc5d4e82a59451f223dfc53e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4e689a6e04b4d41874aaf764d777c2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd334dddf690440ab54308c7629401d3","placeholder":"​","style":"IPY_MODEL_5cef2280c457458eb948b63724542609","value":"100%"}},"c55456a7b618462da542a45964328643":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b04c5010d6be4498af36df0c831b7c61","placeholder":"​","style":"IPY_MODEL_734515401cd14f488860c1bccb5c7144","value":"Downloading: 100%"}},"c96f260a7adf445f898394ae845ed3d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6282e136674cfbbfdd444dd3390475","placeholder":"​","style":"IPY_MODEL_235b85006da24561bf45f61789a133ee","value":"100%"}},"ca026651ce06469c8f757f5a8c4700de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca03003c9d51469cbb3cebd7b1f713a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea73752595ab4ad8aa493b97193b88b4","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10d5aa36dfd3443787a7aeee5161b3c7","value":31}},"ca4faf641c6e41309ccb7de59ce8ffd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cba9f857b31f441fb1c515627a1c8a9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48f6e65569f644c9a832a71735501b8a","IPY_MODEL_7cb9fd3dadcd46ec977d5b9a3a58c0f1","IPY_MODEL_567010dc3dc34d91bf4cd7a752761c09"],"layout":"IPY_MODEL_86f2dbe9725c4274b2ed103b65aeb1b4"}},"cd30630207d9437d8e1c04b7b06ef4db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd52726c54c54878b01eebc00523401a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf8f4c06d8b9475ba91861557fa419fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d172141a8e894262baf7180443081f13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1badb2a2646c4ea785259b8ce3355d6c","placeholder":"​","style":"IPY_MODEL_93b1034edd93482cb08ec43fb303a80b","value":" 779k/779k [00:00&lt;00:00, 921kB/s]"}},"d1b84f858a4047e88580b5a5f19697a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2ef824f280f4aa7ae140bd4960183f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef98240540a84037a68f80d259d4211c","IPY_MODEL_408abacc72a44a97b8564067fcf96143","IPY_MODEL_4715a0f6d0ad4ee480a07f059f82adf2"],"layout":"IPY_MODEL_6dd874080a334f10bfae16878c23ba77"}},"d578f2c9e4e64ab4b2c383889fcab4ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d60de736f10641789969b8a8c26c6b89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6ddd167d85c4c48bb112081a86a1639":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b3584c558254b898013e72e88ffd776","placeholder":"​","style":"IPY_MODEL_659a998199d84fdf929731b704ae79d6","value":"100%"}},"d7abea0b567646d692309dffbe70d0c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c6acfcc13c346a1a0ae364c6d55bb3f","IPY_MODEL_59a7c632cf55450996f0e2a7921404c1","IPY_MODEL_c16ef68e7521495abf00badf2a99d2d6"],"layout":"IPY_MODEL_95e5a5e414f542cfa0de676bc9369e28"}},"da8ad50ad8de4f1b83413937c024634f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db104488ce674ca6a1697f0bb7461e19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db8edd12161b425ea45a877c9bcfbbcd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e28646ec11c04c4b81b627a9caa5badf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e37a3ede985d4d2ea02baea88243c654":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e457aa356e3d40d7a9e364d98fed204e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e891a0014bd34e90a495847943b49e6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8a61f5aed7f4405983b95b0aad316cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e96c861741d04295992e84ada72bc0d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea73752595ab4ad8aa493b97193b88b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef6282e136674cfbbfdd444dd3390475":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef98240540a84037a68f80d259d4211c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4090644364594668a5a1e65ea4d4b3bd","placeholder":"​","style":"IPY_MODEL_6be533055a5146b090dee076c13e145f","value":"Downloading: 100%"}},"f00a4170a26542fc8938a1f0c6ba6fe9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f040058a407a48dd87871fe1dc4c0c30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f319a1047b3c45ab9b6ad98857bcb0fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3250b800fa849bfac9a2904c43b962e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3c671ea857d414cbef33cb2714540f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e39301292f64ae9a96e48713f8e96fa","IPY_MODEL_02f4a8483585406b882932b6b7581c52","IPY_MODEL_1367de35696244f8bb88f980b2566ba2"],"layout":"IPY_MODEL_3587aaa0b37f4646bdb17c1b7b6bd4e4"}},"f6175ffbd35d4f34a57e7abdff62f1ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f99a96b6824347c78a0b661df7e52288":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbf709439dad475d9f0aa4c5236ffd29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_97654dbb9ce3441ab15b069f7cba621a","max":898822,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8df868fa0d894ff882eaac96a1a9e637","value":898822}},"fd334dddf690440ab54308c7629401d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fed9453482bc49f381638182f77f5a3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
