{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test_on_Old_Datatset.ipynb","provenance":[{"file_id":"1wyh0KWu1e3MomRSboWXj2BXLVYJsPuYr","timestamp":1655583814189}],"collapsed_sections":[],"mount_file_id":"1DDnmwjEsvjA0JhfEh9gv1Otf1hcKttgp","authorship_tag":"ABX9TyMHQBHMrrlOBowGJ5Du+aqv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"5ef8e6a8ee864018adc73a448b2ff23e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ed485506c5d4df2807d685cbfd0a337","IPY_MODEL_24c2ee2c36c84f1fa551ecd50978dd7e","IPY_MODEL_3cd50a155fbc425ab3e87e6dc5e92b64"],"layout":"IPY_MODEL_7b189f79104443fb87478c3a42efae5a"}},"0ed485506c5d4df2807d685cbfd0a337":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5711a44382164a608d83aaa2ea5b9049","placeholder":"​","style":"IPY_MODEL_2900511e5016402c9fb7deff1bdea7d3","value":"Downloading: 100%"}},"24c2ee2c36c84f1fa551ecd50978dd7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9181520bdc974a66a3a76057cf46f4c7","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00cb1426cae845de9a0838a27b0f3c24","value":570}},"3cd50a155fbc425ab3e87e6dc5e92b64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98b80bc9e96b4eba9a109bb8c6c761d5","placeholder":"​","style":"IPY_MODEL_3578766244ee4c8881e5271675014f22","value":" 570/570 [00:00&lt;00:00, 3.99kB/s]"}},"7b189f79104443fb87478c3a42efae5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5711a44382164a608d83aaa2ea5b9049":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2900511e5016402c9fb7deff1bdea7d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9181520bdc974a66a3a76057cf46f4c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00cb1426cae845de9a0838a27b0f3c24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98b80bc9e96b4eba9a109bb8c6c761d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3578766244ee4c8881e5271675014f22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ab6e56fe47d472b92e6e4a193a7f09f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6d76f5004c64025a930f1c5dfb9a09a","IPY_MODEL_af8510cfa3654dde9fa20c70570a2245","IPY_MODEL_0ed4737629f546e387b210cb35a5b2d7"],"layout":"IPY_MODEL_4c94e7472e0148a48e97d020df3eb2c0"}},"f6d76f5004c64025a930f1c5dfb9a09a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee7aed28dd83412aa9669ee9aec7c035","placeholder":"​","style":"IPY_MODEL_a3deae920a9047c0ba7c4b75bd317571","value":"Downloading: 100%"}},"af8510cfa3654dde9fa20c70570a2245":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d338b7eb31ad4cbc99138a573027a72c","max":435779157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1cf222e3a44c45518bc317fb05b6fecd","value":435779157}},"0ed4737629f546e387b210cb35a5b2d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1af1353c31d945a38747dab6d3d74d24","placeholder":"​","style":"IPY_MODEL_905e65a41f4846d990800166f344036e","value":" 416M/416M [00:15&lt;00:00, 57.8MB/s]"}},"4c94e7472e0148a48e97d020df3eb2c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee7aed28dd83412aa9669ee9aec7c035":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3deae920a9047c0ba7c4b75bd317571":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d338b7eb31ad4cbc99138a573027a72c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cf222e3a44c45518bc317fb05b6fecd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1af1353c31d945a38747dab6d3d74d24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"905e65a41f4846d990800166f344036e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66f571fe87694f8e89fc5937d0b5c66c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bb549d27c8e44868141c93f945ca514","IPY_MODEL_b87efed6fa534ac6b3187a9309bf899d","IPY_MODEL_44d7784196e64a059758fc19ac5bdfff"],"layout":"IPY_MODEL_4a1966e34753405f9795f10dab68eb42"}},"7bb549d27c8e44868141c93f945ca514":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_545f7d8dd18143298bdb7a2a1e06114a","placeholder":"​","style":"IPY_MODEL_de28372429c04a418db497dec394e21c","value":"Downloading: 100%"}},"b87efed6fa534ac6b3187a9309bf899d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe9bbcdd5eea4649b6fb92460fccb1a1","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf52ae9c24d945d099148a77ca2e5cf1","value":29}},"44d7784196e64a059758fc19ac5bdfff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87fb8fd3d49f44db953d9cf8255cebbc","placeholder":"​","style":"IPY_MODEL_d7a5df6ed708426c9a7fc21a0696902c","value":" 29.0/29.0 [00:00&lt;00:00, 875B/s]"}},"4a1966e34753405f9795f10dab68eb42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"545f7d8dd18143298bdb7a2a1e06114a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de28372429c04a418db497dec394e21c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe9bbcdd5eea4649b6fb92460fccb1a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf52ae9c24d945d099148a77ca2e5cf1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87fb8fd3d49f44db953d9cf8255cebbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7a5df6ed708426c9a7fc21a0696902c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1a958465f864a599abc14382123befd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e14df36dc3b944bf8a5f0f3115708aeb","IPY_MODEL_aa5cac4fb0a64d1eac476365f1942358","IPY_MODEL_9c3b24a658e14654ad783aa1d9c3ea12"],"layout":"IPY_MODEL_5504ec6b68a04c91b90711fdbee043c8"}},"e14df36dc3b944bf8a5f0f3115708aeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0110d77b11b4cf5b680affcf9b08312","placeholder":"​","style":"IPY_MODEL_d3fce426ea83448685dbeb6529ca9b09","value":"Downloading: 100%"}},"aa5cac4fb0a64d1eac476365f1942358":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_678b1fa93c2a40e1a1e409384ec0f335","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68dc87651f7a4f1299ebaefbb6507427","value":213450}},"9c3b24a658e14654ad783aa1d9c3ea12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8a8cc534c424f6e872f89a51cdfac06","placeholder":"​","style":"IPY_MODEL_f90c37f39a41427982db5cc6cc3038da","value":" 208k/208k [00:00&lt;00:00, 873kB/s]"}},"5504ec6b68a04c91b90711fdbee043c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0110d77b11b4cf5b680affcf9b08312":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3fce426ea83448685dbeb6529ca9b09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"678b1fa93c2a40e1a1e409384ec0f335":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68dc87651f7a4f1299ebaefbb6507427":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8a8cc534c424f6e872f89a51cdfac06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f90c37f39a41427982db5cc6cc3038da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3e09daf23e34551a91f6284fa7f8ca3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abbd2e5da5b445f6bdff28b1f828d879","IPY_MODEL_437b79bc3c804230a7d0b80e4c2f260e","IPY_MODEL_123d6c31685b4cb9839621f5e0614656"],"layout":"IPY_MODEL_4efbbb1fd4384be1805c45255ace1a41"}},"abbd2e5da5b445f6bdff28b1f828d879":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c5f69577de74603841d3a56b9071c98","placeholder":"​","style":"IPY_MODEL_1279f435e3c64aa797e748f46d95627d","value":"Downloading: 100%"}},"437b79bc3c804230a7d0b80e4c2f260e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18b2c8b97e9441b8a8f8ef09b3096712","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b5b750fe3a94d9593729381504cba2d","value":435797}},"123d6c31685b4cb9839621f5e0614656":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09259f929d1544f492eb106f3741644a","placeholder":"​","style":"IPY_MODEL_ae6930b764754b4cbe03f36093d6c257","value":" 426k/426k [00:00&lt;00:00, 931kB/s]"}},"4efbbb1fd4384be1805c45255ace1a41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c5f69577de74603841d3a56b9071c98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1279f435e3c64aa797e748f46d95627d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18b2c8b97e9441b8a8f8ef09b3096712":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b5b750fe3a94d9593729381504cba2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09259f929d1544f492eb106f3741644a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae6930b764754b4cbe03f36093d6c257":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69dac67eba6f4e68b3d67314dab8d584":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc6794c2fcaa4aa4a7fb45f28db28d48","IPY_MODEL_5f24126450cb41ffa14d57fbcbfea488","IPY_MODEL_7dc4e1b25df443f68e34aed79a3d302f"],"layout":"IPY_MODEL_ba4c9796fa614682a395b95853b1d015"}},"bc6794c2fcaa4aa4a7fb45f28db28d48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d45c647b16b42048a4b8a3d6d5006eb","placeholder":"​","style":"IPY_MODEL_020cdf8d66d24c3589ed771c701ba0bd","value":"100%"}},"5f24126450cb41ffa14d57fbcbfea488":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3511320c8a6a469ca2754299f7c95464","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fb790f6ca44456da775bb7fe544cfde","value":8}},"7dc4e1b25df443f68e34aed79a3d302f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3d4f936b3b340e8b5726f31c5f90076","placeholder":"​","style":"IPY_MODEL_89d994937ae9413bbf7197e34dbfe77f","value":" 8/8 [00:01&lt;00:00,  3.98ba/s]"}},"ba4c9796fa614682a395b95853b1d015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d45c647b16b42048a4b8a3d6d5006eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"020cdf8d66d24c3589ed771c701ba0bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3511320c8a6a469ca2754299f7c95464":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fb790f6ca44456da775bb7fe544cfde":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3d4f936b3b340e8b5726f31c5f90076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89d994937ae9413bbf7197e34dbfe77f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b94a8ba3946448a69b01c480e7c16a6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27ecb88e32ad4b23aee006a16e380297","IPY_MODEL_624e0b653d134daba688fd018d1a013c","IPY_MODEL_a3d032acec384a209171f1e8917e5cb0"],"layout":"IPY_MODEL_0188a9ed71b44abbb4a97cf97343f88e"}},"27ecb88e32ad4b23aee006a16e380297":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2ec12bdea3d422b9aef2636f5f36338","placeholder":"​","style":"IPY_MODEL_9ae23292751a4c08a03398a6c9f43e6f","value":"Downloading: 100%"}},"624e0b653d134daba688fd018d1a013c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc024a7c57264777bb3e4ff987442993","max":1045,"min":0,"orientation":"horizontal","style":"IPY_MODEL_120350f205fe478b9bf73aa41c5229f9","value":1045}},"a3d032acec384a209171f1e8917e5cb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3c9c64e53fc4a49814ba362adb80ba9","placeholder":"​","style":"IPY_MODEL_afd5cbca60e04f0791c22108369d6a93","value":" 1.02k/1.02k [00:00&lt;00:00, 23.8kB/s]"}},"0188a9ed71b44abbb4a97cf97343f88e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2ec12bdea3d422b9aef2636f5f36338":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ae23292751a4c08a03398a6c9f43e6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc024a7c57264777bb3e4ff987442993":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"120350f205fe478b9bf73aa41c5229f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3c9c64e53fc4a49814ba362adb80ba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afd5cbca60e04f0791c22108369d6a93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d39d6bc3305b45bbac7cd700fc53d858":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ed9e911e6d34251ab53026bcd84a989","IPY_MODEL_7fb53a41b59844acb75fa96ef2fff0df","IPY_MODEL_a45e9c2303cf47b593ed773fea461175"],"layout":"IPY_MODEL_fbf64718c95341d28b4ab82b4cb09b40"}},"3ed9e911e6d34251ab53026bcd84a989":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fc3b3d8e8b04668b1f0ab1f195f0e43","placeholder":"​","style":"IPY_MODEL_3e63c88862734723993f8d7971303143","value":"Downloading: 100%"}},"7fb53a41b59844acb75fa96ef2fff0df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2174f03756b041f2867a924b726d088e","max":560,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df8110f66d034be082d6093d6132adee","value":560}},"a45e9c2303cf47b593ed773fea461175":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b062dc6fbe04ef8b47ef0d8786946a5","placeholder":"​","style":"IPY_MODEL_8353d74ca1b947dc81876bc9482dd956","value":" 560/560 [00:00&lt;00:00, 12.5kB/s]"}},"fbf64718c95341d28b4ab82b4cb09b40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fc3b3d8e8b04668b1f0ab1f195f0e43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e63c88862734723993f8d7971303143":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2174f03756b041f2867a924b726d088e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df8110f66d034be082d6093d6132adee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b062dc6fbe04ef8b47ef0d8786946a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8353d74ca1b947dc81876bc9482dd956":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70207495bb9c4dc0a3bf6e9af71257da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_891e3acf57c3490cb76fe15dd5802530","IPY_MODEL_91aef6ae96314fffaa2697b5b3f105d3","IPY_MODEL_c3ad9432db064daa8fad2ca8ba6252ae"],"layout":"IPY_MODEL_e8f8a79cfd7d456bae62ce388a2aa675"}},"891e3acf57c3490cb76fe15dd5802530":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d92f805e39940ad91e08d5a305d2368","placeholder":"​","style":"IPY_MODEL_c49608e7f6d6438ca4e621f6efbe4f77","value":"Downloading: 100%"}},"91aef6ae96314fffaa2697b5b3f105d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eb9c09513fc41b491d7f7e7b548d06c","max":898822,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1eea29d6f03944b2a19b6c52a2843d56","value":898822}},"c3ad9432db064daa8fad2ca8ba6252ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d51291c7be95414bb2ab723ce459151f","placeholder":"​","style":"IPY_MODEL_e4bb1eedf686417a889f8ce1a5aa1da6","value":" 878k/878k [00:00&lt;00:00, 3.01MB/s]"}},"e8f8a79cfd7d456bae62ce388a2aa675":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d92f805e39940ad91e08d5a305d2368":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c49608e7f6d6438ca4e621f6efbe4f77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7eb9c09513fc41b491d7f7e7b548d06c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eea29d6f03944b2a19b6c52a2843d56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d51291c7be95414bb2ab723ce459151f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4bb1eedf686417a889f8ce1a5aa1da6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c7166447b034dd5bf6c89abc47b8082":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5131de01dfd847fc8451dc970e166cca","IPY_MODEL_0d0095511b85452d9217045280b33a90","IPY_MODEL_cc0625add6474b00a056e23e2c846bdd"],"layout":"IPY_MODEL_efdd9ed62d8341f6b71e83cb0b5d337c"}},"5131de01dfd847fc8451dc970e166cca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f01dc5d3d2984fcc910e477ce07004fb","placeholder":"​","style":"IPY_MODEL_c0baceded4464f3ca7e4087e13d89c2e","value":"Downloading: 100%"}},"0d0095511b85452d9217045280b33a90":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e6c0f6f70d54e5e86f92574dc449489","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aad5e5189ac4462b8f5290431f2c6c13","value":456318}},"cc0625add6474b00a056e23e2c846bdd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c67a39ba35974912b42966ba5bf748c3","placeholder":"​","style":"IPY_MODEL_fb7d09ff2cb4461f9a7c60601bf3b399","value":" 446k/446k [00:00&lt;00:00, 970kB/s]"}},"efdd9ed62d8341f6b71e83cb0b5d337c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f01dc5d3d2984fcc910e477ce07004fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0baceded4464f3ca7e4087e13d89c2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e6c0f6f70d54e5e86f92574dc449489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aad5e5189ac4462b8f5290431f2c6c13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c67a39ba35974912b42966ba5bf748c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb7d09ff2cb4461f9a7c60601bf3b399":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ef2036da34643f78fd2fbb4937485d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59917414af2046faaf4c3576e6267156","IPY_MODEL_a42164c3a5804721b4593ee9942d0680","IPY_MODEL_0092ea68b865432d976b5f59af02ced0"],"layout":"IPY_MODEL_2b0e9cc25798469cabc01448c408da0c"}},"59917414af2046faaf4c3576e6267156":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59ea7f7f352c4f48a2b415433717c99e","placeholder":"​","style":"IPY_MODEL_ba401d6427cc4505a13b85df050762bc","value":"Downloading: 100%"}},"a42164c3a5804721b4593ee9942d0680":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2ba677a54f849a8a14d81cf6d2d2d98","max":772,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18b0ce097c064e8fa45b44fffb78a64d","value":772}},"0092ea68b865432d976b5f59af02ced0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be78983349e046fcb13656fa3cd25c90","placeholder":"​","style":"IPY_MODEL_69e2778d0a5f4e048e9238310b9575fa","value":" 772/772 [00:00&lt;00:00, 21.9kB/s]"}},"2b0e9cc25798469cabc01448c408da0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59ea7f7f352c4f48a2b415433717c99e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba401d6427cc4505a13b85df050762bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2ba677a54f849a8a14d81cf6d2d2d98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18b0ce097c064e8fa45b44fffb78a64d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be78983349e046fcb13656fa3cd25c90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69e2778d0a5f4e048e9238310b9575fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecd547b295fb4d17a397e151438e5cdd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ddb93bfc0df47f3b04ff199f3eaacd7","IPY_MODEL_be04604756bb4c8288fc899da78cd7af","IPY_MODEL_8d72c3d376214203a8306026115320b7"],"layout":"IPY_MODEL_29bc50bb883441a180e978ebaa4f52a0"}},"9ddb93bfc0df47f3b04ff199f3eaacd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9cd5ae4862c48babf1fde0aaf8937fb","placeholder":"​","style":"IPY_MODEL_67f3a618d8f6477ca54e0d1d0c7c6bba","value":"Downloading: 100%"}},"be04604756bb4c8288fc899da78cd7af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e115694c5204eaa97e2c903112581d8","max":498677330,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6eb29b6bbf654d8caf0152fe5b835bcd","value":498677330}},"8d72c3d376214203a8306026115320b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4cb363e1f1843679c1ddcdbb47a52a0","placeholder":"​","style":"IPY_MODEL_28852efc0eda416cb838e3503bed8558","value":" 476M/476M [00:13&lt;00:00, 46.4MB/s]"}},"29bc50bb883441a180e978ebaa4f52a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9cd5ae4862c48babf1fde0aaf8937fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67f3a618d8f6477ca54e0d1d0c7c6bba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e115694c5204eaa97e2c903112581d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6eb29b6bbf654d8caf0152fe5b835bcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4cb363e1f1843679c1ddcdbb47a52a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28852efc0eda416cb838e3503bed8558":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6511f0a6072f4b33af472e7843859501":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4312a6613b9c40288cba76688ed36f03","IPY_MODEL_2a10544860d74740843da08066e08897","IPY_MODEL_9709848f57494681ba950187c3424886"],"layout":"IPY_MODEL_23539c23c9a1428094474c8163c412d3"}},"4312a6613b9c40288cba76688ed36f03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f825a23df66b406b9ae39843ade96a94","placeholder":"​","style":"IPY_MODEL_e877c56535ad470989bcb239f2fa5dee","value":"100%"}},"2a10544860d74740843da08066e08897":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef4e51c7f250492496e8b627638eea15","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_945feca29c3147ea9a2c0f422c4dbee8","value":8}},"9709848f57494681ba950187c3424886":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb2c26b6db90424e96f355ef362a062f","placeholder":"​","style":"IPY_MODEL_83e6ac54b0e642468d9599b9f38d1e47","value":" 8/8 [00:01&lt;00:00,  5.17ba/s]"}},"23539c23c9a1428094474c8163c412d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f825a23df66b406b9ae39843ade96a94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e877c56535ad470989bcb239f2fa5dee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef4e51c7f250492496e8b627638eea15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"945feca29c3147ea9a2c0f422c4dbee8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb2c26b6db90424e96f355ef362a062f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83e6ac54b0e642468d9599b9f38d1e47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be228ad6ff294808917055ef4701e351":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c16db2e032564355910bfc923ff85f7e","IPY_MODEL_852520aef9c44b46b0bef18e943d5031","IPY_MODEL_aa86c57bcdc14366938c6db0d023aa69"],"layout":"IPY_MODEL_0bd24129c7ec4f91a181dde8e5af60df"}},"c16db2e032564355910bfc923ff85f7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15d45324e855461797ddaaf8fc1a6df4","placeholder":"​","style":"IPY_MODEL_375c98399ec847f1ab245e85edd1791b","value":"Downloading: 100%"}},"852520aef9c44b46b0bef18e943d5031":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c953e903ab1641a1809e6872f79a4674","max":1140,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac43b4fa05844d6896967226b8c50cae","value":1140}},"aa86c57bcdc14366938c6db0d023aa69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7591245943aa4abcaac089497f5ffc0b","placeholder":"​","style":"IPY_MODEL_7ed5995ce33547678fd46e2d9363abbf","value":" 1.11k/1.11k [00:00&lt;00:00, 37.1kB/s]"}},"0bd24129c7ec4f91a181dde8e5af60df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15d45324e855461797ddaaf8fc1a6df4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"375c98399ec847f1ab245e85edd1791b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c953e903ab1641a1809e6872f79a4674":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac43b4fa05844d6896967226b8c50cae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7591245943aa4abcaac089497f5ffc0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ed5995ce33547678fd46e2d9363abbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7295b1ed25e4c4085382d85f27afeda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_259fe934f1d44006ac26dfce3611f744","IPY_MODEL_cab50387e209487bbdadf2a4ece16477","IPY_MODEL_a5b43800429c4410ab51fcdb75d2a5b9"],"layout":"IPY_MODEL_ffe6df3bec1e44909b8d1b48a338341a"}},"259fe934f1d44006ac26dfce3611f744":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_223521de6516416c91ddb1d1198ae915","placeholder":"​","style":"IPY_MODEL_1a538b36b6b547a5a5cfe2deb63eb995","value":"Downloading: 100%"}},"cab50387e209487bbdadf2a4ece16477":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_353de2cf1d7f47f3a1265ba90f1fd3f5","max":898822,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fce6b67d5fee4c5d80cdb586d29feaa9","value":898822}},"a5b43800429c4410ab51fcdb75d2a5b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c4686aec10e40918b90d3590d45228b","placeholder":"​","style":"IPY_MODEL_a2f5cc927290403c8a0e3f8b2ed832de","value":" 878k/878k [00:00&lt;00:00, 1.07MB/s]"}},"ffe6df3bec1e44909b8d1b48a338341a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"223521de6516416c91ddb1d1198ae915":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a538b36b6b547a5a5cfe2deb63eb995":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"353de2cf1d7f47f3a1265ba90f1fd3f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fce6b67d5fee4c5d80cdb586d29feaa9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c4686aec10e40918b90d3590d45228b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2f5cc927290403c8a0e3f8b2ed832de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6b31cfb4f9147888bf736ffb98f6a0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4aa0ae98769416283cd5bd6dd24e0cc","IPY_MODEL_790acb88c8994430b44e943ec92e86bb","IPY_MODEL_d7e2eab7feea4ef8b18ab9443bfdaf37"],"layout":"IPY_MODEL_77f6dda8086541988e5b42900e72fddf"}},"b4aa0ae98769416283cd5bd6dd24e0cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ab3470e97f54b25b488837a3f2c37ce","placeholder":"​","style":"IPY_MODEL_6ea0bb273355444ab5d92c4d42807a84","value":"Downloading: 100%"}},"790acb88c8994430b44e943ec92e86bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a680303e81bb4ed9acf2850daa64f05c","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e556975be9b4d0985b25abdb0591bc0","value":456318}},"d7e2eab7feea4ef8b18ab9443bfdaf37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f8d8a1c66343d69d201050ee80a974","placeholder":"​","style":"IPY_MODEL_860b056e72a548a08e34087bfa05b622","value":" 446k/446k [00:00&lt;00:00, 1.07MB/s]"}},"77f6dda8086541988e5b42900e72fddf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ab3470e97f54b25b488837a3f2c37ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ea0bb273355444ab5d92c4d42807a84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a680303e81bb4ed9acf2850daa64f05c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e556975be9b4d0985b25abdb0591bc0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4f8d8a1c66343d69d201050ee80a974":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"860b056e72a548a08e34087bfa05b622":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"354d4958f8474884ab81ad8aac60c76a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_faea05e52e5d4fab89844f66e51df40a","IPY_MODEL_224b5622cdd1482d9b036ab2c11a00bb","IPY_MODEL_8726221f024b4d21b0159685426e9bfa"],"layout":"IPY_MODEL_5e026f7f35664a85ad2eb9948f6d12e0"}},"faea05e52e5d4fab89844f66e51df40a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a78e5183a69e437585eea69933656999","placeholder":"​","style":"IPY_MODEL_98a0b27f8a2a4b76b34723126a1cdab6","value":"Downloading: 100%"}},"224b5622cdd1482d9b036ab2c11a00bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c71b5445017b457bb3ce63f8c6b61ea8","max":772,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c00b95ef57794ca7b98e8946394fa5fc","value":772}},"8726221f024b4d21b0159685426e9bfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00fdd87c5c1647e784afd3138a60bb6a","placeholder":"​","style":"IPY_MODEL_ef06f2a6306f44c9acb6c5caa81bd838","value":" 772/772 [00:00&lt;00:00, 23.9kB/s]"}},"5e026f7f35664a85ad2eb9948f6d12e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a78e5183a69e437585eea69933656999":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98a0b27f8a2a4b76b34723126a1cdab6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c71b5445017b457bb3ce63f8c6b61ea8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c00b95ef57794ca7b98e8946394fa5fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"00fdd87c5c1647e784afd3138a60bb6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef06f2a6306f44c9acb6c5caa81bd838":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f7a20a13e204b32973f10d40a322665":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b98c5b8bdcb4574b7fe21154f244879","IPY_MODEL_efd6489798814c04b08390e7e3f88928","IPY_MODEL_012b50c01cb84118878f27b0dcc145ca"],"layout":"IPY_MODEL_8d8141c421b644739048017dfb3dfcb0"}},"6b98c5b8bdcb4574b7fe21154f244879":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_051ee33cbff94a7b8bbb62f4b723826d","placeholder":"​","style":"IPY_MODEL_73e77319cc1a4cb7b33431801afc6a5c","value":"Downloading: 100%"}},"efd6489798814c04b08390e7e3f88928":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d4488ec3993495798efc4599d4d29cd","max":735,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a9f0b117dc14063b6d67e4788c80196","value":735}},"012b50c01cb84118878f27b0dcc145ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99943779e7ea4a57ab0c2e630e6d8bb2","placeholder":"​","style":"IPY_MODEL_4b17c1a91a0249de843b537504aeef85","value":" 735/735 [00:00&lt;00:00, 26.0kB/s]"}},"8d8141c421b644739048017dfb3dfcb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"051ee33cbff94a7b8bbb62f4b723826d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73e77319cc1a4cb7b33431801afc6a5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d4488ec3993495798efc4599d4d29cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a9f0b117dc14063b6d67e4788c80196":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99943779e7ea4a57ab0c2e630e6d8bb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b17c1a91a0249de843b537504aeef85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5f352de766d4d6c9e55f7d5e54897de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9552dca948f400b842bab623f697a70","IPY_MODEL_10569ed12e6749ccaa0895f13641a68f","IPY_MODEL_ccb374cd03a04eefb73bebe1a5b60d14"],"layout":"IPY_MODEL_f4134ea87e504d86a7129c392a4cb851"}},"a9552dca948f400b842bab623f697a70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f401ae7c960948138fb4a2c3e1cccede","placeholder":"​","style":"IPY_MODEL_b917878d61824077bd99e4ac94ec1689","value":"Downloading: 100%"}},"10569ed12e6749ccaa0895f13641a68f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfef79c03d454dfea4fadc098b055c4f","max":498657517,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d4241d68ce64882afc839fb1c007d54","value":498657517}},"ccb374cd03a04eefb73bebe1a5b60d14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c81fbe298654f87b866c601deef10c3","placeholder":"​","style":"IPY_MODEL_41b9e789e078456a95969dfa4f5d9fac","value":" 476M/476M [00:12&lt;00:00, 38.3MB/s]"}},"f4134ea87e504d86a7129c392a4cb851":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f401ae7c960948138fb4a2c3e1cccede":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b917878d61824077bd99e4ac94ec1689":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfef79c03d454dfea4fadc098b055c4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d4241d68ce64882afc839fb1c007d54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c81fbe298654f87b866c601deef10c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b9e789e078456a95969dfa4f5d9fac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91f96c7ffcad42f79d8fc1df3e5ac280":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f29979a52448481b99f549e15f993275","IPY_MODEL_dae3562b1aeb40ecbd88f9933b05d7a9","IPY_MODEL_bb3faea83fcf4264a53d91513b7c2388"],"layout":"IPY_MODEL_b4ef6c7d956447a59c4849f8b1bf4a20"}},"f29979a52448481b99f549e15f993275":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20888573be8d4ef6815686d4c15da8cc","placeholder":"​","style":"IPY_MODEL_8b18b0f534984af0bf7320cc2ba8a285","value":"100%"}},"dae3562b1aeb40ecbd88f9933b05d7a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_329ce19acf2e4640923e25d234ac2a68","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ace33fbd0864a69ac3aed4db1a5e0e5","value":8}},"bb3faea83fcf4264a53d91513b7c2388":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_741efd22cd8946f58407cd9521bc74e8","placeholder":"​","style":"IPY_MODEL_3e331f1e8b2746cbb4660625b0d7a1c0","value":" 8/8 [00:01&lt;00:00,  5.30ba/s]"}},"b4ef6c7d956447a59c4849f8b1bf4a20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20888573be8d4ef6815686d4c15da8cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b18b0f534984af0bf7320cc2ba8a285":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"329ce19acf2e4640923e25d234ac2a68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ace33fbd0864a69ac3aed4db1a5e0e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"741efd22cd8946f58407cd9521bc74e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e331f1e8b2746cbb4660625b0d7a1c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ada4112560c40f193123ce48714aee9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4038f26e2d794226ac4badc90ffb7ef9","IPY_MODEL_864da61cc7d54e8dac72735dcbdaf871","IPY_MODEL_383c7c1ad3b446e0b8edef4b63115121"],"layout":"IPY_MODEL_c1a8b5f114ad4dedbfcd03759db63c6e"}},"4038f26e2d794226ac4badc90ffb7ef9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bfba27eb346483eba83cafa1b2f918d","placeholder":"​","style":"IPY_MODEL_99ae3e3e4d3a4342a07ff4be484f7b94","value":"100%"}},"864da61cc7d54e8dac72735dcbdaf871":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_99db5d6e926f4734a8043d2c6e83303e","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e60eea3280f48289ae2fcd32d1c5cc3","value":8}},"383c7c1ad3b446e0b8edef4b63115121":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efbbf0dbf48c44639540c2241a952156","placeholder":"​","style":"IPY_MODEL_95013866cd824ab99160898f6d600d22","value":" 8/8 [00:01&lt;00:00,  5.33ba/s]"}},"c1a8b5f114ad4dedbfcd03759db63c6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bfba27eb346483eba83cafa1b2f918d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99ae3e3e4d3a4342a07ff4be484f7b94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99db5d6e926f4734a8043d2c6e83303e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e60eea3280f48289ae2fcd32d1c5cc3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"efbbf0dbf48c44639540c2241a952156":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95013866cd824ab99160898f6d600d22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61d484a9d2654b2590e79b8ce5fc2114":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b62d699eff324db4bcf24b241dab3c71","IPY_MODEL_7d94bcb666364b7787b2637bbc4dea4b","IPY_MODEL_d5557d241b5443ab8af249c44a23ff95"],"layout":"IPY_MODEL_e8f722eba94e46c0a36e84f271d85979"}},"b62d699eff324db4bcf24b241dab3c71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa942a86dca94cba9401edc0d6ae7ffb","placeholder":"​","style":"IPY_MODEL_7b8bb767d66d479aa0e66858ccc958cb","value":"100%"}},"7d94bcb666364b7787b2637bbc4dea4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_938a26269910440f8459d2aca0eaba30","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ccaa6825116846fc897135b3b1625b19","value":8}},"d5557d241b5443ab8af249c44a23ff95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_411827da8e44453f9495e825e99a84dc","placeholder":"​","style":"IPY_MODEL_24e03d825dde46fe92257a01f7c38c5c","value":" 8/8 [00:01&lt;00:00,  5.23ba/s]"}},"e8f722eba94e46c0a36e84f271d85979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa942a86dca94cba9401edc0d6ae7ffb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b8bb767d66d479aa0e66858ccc958cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"938a26269910440f8459d2aca0eaba30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccaa6825116846fc897135b3b1625b19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"411827da8e44453f9495e825e99a84dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24e03d825dde46fe92257a01f7c38c5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a01f4b208b44c618422bb7198f36cd6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1262fd3fc1394c5d8d5fdd07a50f0a0d","IPY_MODEL_138b01af1a654589b1247e3de1a4c842","IPY_MODEL_77e09f6a90464635a4eb1d1d09a614f5"],"layout":"IPY_MODEL_3d5ced7af2c14864b673d2ad78c565b8"}},"1262fd3fc1394c5d8d5fdd07a50f0a0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8523a63fdb24ed0a4bc6fc2a2fc7a4f","placeholder":"​","style":"IPY_MODEL_43d0af299130495e92278cec0a84ddd4","value":"Downloading: 100%"}},"138b01af1a654589b1247e3de1a4c842":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d5b452d2b1744a294ece929bc9b35f8","max":467042463,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6b78771d7bf4bea8acd999f589c4b21","value":467042463}},"77e09f6a90464635a4eb1d1d09a614f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e8a0de9e2ac4f43ba2bbf121bc96e22","placeholder":"​","style":"IPY_MODEL_9b015486aa5949f2989b20929ecef4a5","value":" 445M/445M [00:09&lt;00:00, 45.1MB/s]"}},"3d5ced7af2c14864b673d2ad78c565b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8523a63fdb24ed0a4bc6fc2a2fc7a4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43d0af299130495e92278cec0a84ddd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d5b452d2b1744a294ece929bc9b35f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6b78771d7bf4bea8acd999f589c4b21":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e8a0de9e2ac4f43ba2bbf121bc96e22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b015486aa5949f2989b20929ecef4a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"MUh8Bahf8U6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_external = \"/content/drive/MyDrive/CoVerifi&MedVerifi/\" # Replace with appropriate path\n","dfTotal_external = pd.read_csv(path_external+\"Combined_News.csv\", usecols=['title', 'label'])\n","\n","def binaryLabel(label):\n","  if label == \"TRUE\":\n","    return 1\n","  return 0\n","dfTotal_external = dfTotal_external.dropna()\n","dfTotal_external['label'] = dfTotal_external['label'].apply(lambda label: binaryLabel(str(label)))"],"metadata":{"id":"yGLQdEfBfHpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAb5d0IR857u","outputId":"34a7d01f-bbeb-4a7a-f863-60d933871ee0","executionInfo":{"status":"ok","timestamp":1655584428645,"user_tz":300,"elapsed":17697,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 53.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.9 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 31.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n","\u001b[K     |████████████████████████████████| 362 kB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 64.6 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 67.3 MB/s \n","\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 59.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 76.5 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 59.4 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 64.5 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.3.2 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/CoVerifi&MedVerifi/CoVerifi/CoVerifi/ML_Model/'"],"metadata":{"id":"3ztjffqHlXlM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"metadata":{"id":"rbCsY25B82OK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","import pandas as pd\n","from datasets import Dataset\n","from datasets import load_metric\n","from transformers import AutoTokenizer\n","import numpy as np\n","from datasets import load_metric\n","from transformers import Trainer\n","from transformers import TrainingArguments\n","\n","#Eval Parameters\n","metric = load_metric(\"accuracy\")\n","test_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n"],"metadata":{"id":"mqVXF-4xu6ZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Validate on BERT"],"metadata":{"id":"MwpKjHAB8az7"}},{"cell_type":"markdown","source":["## Trained on FNN&CoAID"],"metadata":{"id":"aAA6N_G_94eT"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","import pandas as pd\n","from datasets import Dataset\n","from datasets import load_metric\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n","\n","tokenizer(\"Attention is all you need\")\n","\n","def tokenize_data(example):\n","    return tokenizer(example['title'], padding='max_length')\n"],"metadata":{"id":"awFHhnWw8Tk-","colab":{"base_uri":"https://localhost:8080/","height":308,"referenced_widgets":["5ef8e6a8ee864018adc73a448b2ff23e","0ed485506c5d4df2807d685cbfd0a337","24c2ee2c36c84f1fa551ecd50978dd7e","3cd50a155fbc425ab3e87e6dc5e92b64","7b189f79104443fb87478c3a42efae5a","5711a44382164a608d83aaa2ea5b9049","2900511e5016402c9fb7deff1bdea7d3","9181520bdc974a66a3a76057cf46f4c7","00cb1426cae845de9a0838a27b0f3c24","98b80bc9e96b4eba9a109bb8c6c761d5","3578766244ee4c8881e5271675014f22","6ab6e56fe47d472b92e6e4a193a7f09f","f6d76f5004c64025a930f1c5dfb9a09a","af8510cfa3654dde9fa20c70570a2245","0ed4737629f546e387b210cb35a5b2d7","4c94e7472e0148a48e97d020df3eb2c0","ee7aed28dd83412aa9669ee9aec7c035","a3deae920a9047c0ba7c4b75bd317571","d338b7eb31ad4cbc99138a573027a72c","1cf222e3a44c45518bc317fb05b6fecd","1af1353c31d945a38747dab6d3d74d24","905e65a41f4846d990800166f344036e","66f571fe87694f8e89fc5937d0b5c66c","7bb549d27c8e44868141c93f945ca514","b87efed6fa534ac6b3187a9309bf899d","44d7784196e64a059758fc19ac5bdfff","4a1966e34753405f9795f10dab68eb42","545f7d8dd18143298bdb7a2a1e06114a","de28372429c04a418db497dec394e21c","fe9bbcdd5eea4649b6fb92460fccb1a1","bf52ae9c24d945d099148a77ca2e5cf1","87fb8fd3d49f44db953d9cf8255cebbc","d7a5df6ed708426c9a7fc21a0696902c","f1a958465f864a599abc14382123befd","e14df36dc3b944bf8a5f0f3115708aeb","aa5cac4fb0a64d1eac476365f1942358","9c3b24a658e14654ad783aa1d9c3ea12","5504ec6b68a04c91b90711fdbee043c8","b0110d77b11b4cf5b680affcf9b08312","d3fce426ea83448685dbeb6529ca9b09","678b1fa93c2a40e1a1e409384ec0f335","68dc87651f7a4f1299ebaefbb6507427","b8a8cc534c424f6e872f89a51cdfac06","f90c37f39a41427982db5cc6cc3038da","f3e09daf23e34551a91f6284fa7f8ca3","abbd2e5da5b445f6bdff28b1f828d879","437b79bc3c804230a7d0b80e4c2f260e","123d6c31685b4cb9839621f5e0614656","4efbbb1fd4384be1805c45255ace1a41","6c5f69577de74603841d3a56b9071c98","1279f435e3c64aa797e748f46d95627d","18b2c8b97e9441b8a8f8ef09b3096712","1b5b750fe3a94d9593729381504cba2d","09259f929d1544f492eb106f3741644a","ae6930b764754b4cbe03f36093d6c257"]},"executionInfo":{"status":"ok","timestamp":1655584459534,"user_tz":300,"elapsed":30897,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"d36da0bf-6459-4bec-cd10-3bafdd5a0bb1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef8e6a8ee864018adc73a448b2ff23e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ab6e56fe47d472b92e6e4a193a7f09f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66f571fe87694f8e89fc5937d0b5c66c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1a958465f864a599abc14382123befd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e09daf23e34551a91f6284fa7f8ca3"}},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","from datasets import load_metric\n","from transformers import Trainer\n","from transformers import TrainingArguments\n","\n","#Load saved model\n","import torch\n","model.load_state_dict(torch.load(model_path+'finetune_bert_on_CoAID_and_all_of_FNN.pt'))\n","\n","#Dataset\n","dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)\n","\n","#Eval Parameters\n","metric = load_metric(\"accuracy\")\n","test_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"Xnvl8Nca84cX","colab":{"base_uri":"https://localhost:8080/","height":400,"referenced_widgets":["69dac67eba6f4e68b3d67314dab8d584","bc6794c2fcaa4aa4a7fb45f28db28d48","5f24126450cb41ffa14d57fbcbfea488","7dc4e1b25df443f68e34aed79a3d302f","ba4c9796fa614682a395b95853b1d015","7d45c647b16b42048a4b8a3d6d5006eb","020cdf8d66d24c3589ed771c701ba0bd","3511320c8a6a469ca2754299f7c95464","2fb790f6ca44456da775bb7fe544cfde","f3d4f936b3b340e8b5726f31c5f90076","89d994937ae9413bbf7197e34dbfe77f"]},"executionInfo":{"status":"ok","timestamp":1655584851869,"user_tz":300,"elapsed":135518,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"f8511d9b-5603-4ed6-ef34-dafafb5b53a5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69dac67eba6f4e68b3d67314dab8d584"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.8262717868782768,\n"," 'eval_f1': 0.7688536953242836,\n"," 'eval_loss': 0.7342257499694824,\n"," 'eval_precision': 0.9572769953051643,\n"," 'eval_recall': 0.6424070573408948,\n"," 'eval_runtime': 132.534,\n"," 'eval_samples_per_second': 53.247,\n"," 'eval_steps_per_second': 6.662}"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["## Out of box"],"metadata":{"id":"m7Zv7S-4_ie9"}},{"cell_type":"code","source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"Z2BanEDG_hC_","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655584986040,"user_tz":300,"elapsed":134188,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"feaf31c9-02ab-46ba-e03a-2953531de850"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.5502338104010203,\n"," 'eval_f1': 0.0,\n"," 'eval_loss': 0.7079910635948181,\n"," 'eval_precision': 0.0,\n"," 'eval_recall': 0.0,\n"," 'eval_runtime': 131.6827,\n"," 'eval_samples_per_second': 53.591,\n"," 'eval_steps_per_second': 6.706}"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## finetune_bert_on_CoAID&GossipCop_only"],"metadata":{"id":"RHT4xiF99-Vb"}},{"cell_type":"code","source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_CoAID&GossipCop_only.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"s0aWqvP_925G","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655585126534,"user_tz":300,"elapsed":140499,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"68597d46-ed0c-4281-a638-95149b9bdd0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.8136601955505172,\n"," 'eval_f1': 0.7464815885868517,\n"," 'eval_loss': 0.9527012705802917,\n"," 'eval_precision': 0.9617486338797814,\n"," 'eval_recall': 0.6099558916194077,\n"," 'eval_runtime': 131.5944,\n"," 'eval_samples_per_second': 53.627,\n"," 'eval_steps_per_second': 6.71}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## CoAID "],"metadata":{"id":"jpzFJI7Ynb0j"}},{"cell_type":"code","source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_CoAID.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"iVDlKcZWnd7y","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655585266458,"user_tz":300,"elapsed":139930,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"a45cd4af-2c0e-47ab-91f3-6690d2641edb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.9223466062066034,\n"," 'eval_f1': 0.9073385187690227,\n"," 'eval_loss': 0.4804149270057678,\n"," 'eval_precision': 0.9791970802919708,\n"," 'eval_recall': 0.8453056080655325,\n"," 'eval_runtime': 131.5581,\n"," 'eval_samples_per_second': 53.642,\n"," 'eval_steps_per_second': 6.712}"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## finetune_bert_on_CoAID&PolitiFact_only"],"metadata":{"id":"-meGXc9Y-0cF"}},{"cell_type":"code","source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_CoAID&PolitiFact_only.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"HlF-0xTm-xlJ","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655585406078,"user_tz":300,"elapsed":139641,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"45681fa2-368a-4d0c-ed42-7f42748657a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.7992064616692646,\n"," 'eval_f1': 0.7162026837572602,\n"," 'eval_loss': 1.113200306892395,\n"," 'eval_precision': 0.982957669048928,\n"," 'eval_recall': 0.5633270321361059,\n"," 'eval_runtime': 131.7258,\n"," 'eval_samples_per_second': 53.573,\n"," 'eval_steps_per_second': 6.703}"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## finetune_bert_on_all_of_FNN.pt"],"metadata":{"id":"MqWbBiOm-8oF"}},{"cell_type":"code","source":["#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_all_of_FNN.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"LBmv9U_6-71w","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655585543861,"user_tz":300,"elapsed":137793,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"24b30e53-89e6-40c9-dec7-953b3671ef92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.5359217797931132,\n"," 'eval_f1': 0.5065541660388729,\n"," 'eval_loss': 1.7433199882507324,\n"," 'eval_precision': 0.48541726826451054,\n"," 'eval_recall': 0.5296156269691241,\n"," 'eval_runtime': 131.5365,\n"," 'eval_samples_per_second': 53.65,\n"," 'eval_steps_per_second': 6.713}"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## finetune_bert_on_GossipCop_only.pt"],"metadata":{"id":"rE_pP9jp_Eeh"}},{"cell_type":"code","source":["import torch\n","#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_GossipCop_only.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"UZtFCtyz_Dwc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655585682975,"user_tz":300,"elapsed":139121,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"b65066f0-500a-4ec1-9e3f-d7547a40aa31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.3661612583250673,\n"," 'eval_f1': 0.3895182202811519,\n"," 'eval_loss': 2.2356884479522705,\n"," 'eval_precision': 0.34360703106188295,\n"," 'eval_recall': 0.4495904221802142,\n"," 'eval_runtime': 131.5074,\n"," 'eval_samples_per_second': 53.662,\n"," 'eval_steps_per_second': 6.714}"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["##finetune_bert_on_PolitiFact_only"],"metadata":{"id":"mn3agbV__R6T"}},{"cell_type":"code","source":["\n","import torch\n","#Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n","\n","#Load saved model\n","model_name = 'finetune_bert_on_PolitiFact_only.pt'\n","model.load_state_dict(torch.load(model_path+model_name))\n","\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"0teMWm0R_Qmy","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655585824028,"user_tz":300,"elapsed":141072,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"d286be95-4121-423d-e9c6-680961a9d579"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.7092248830947995,\n"," 'eval_f1': 0.5672711935892029,\n"," 'eval_loss': 1.2808611392974854,\n"," 'eval_precision': 0.857780612244898,\n"," 'eval_recall': 0.423755513547574,\n"," 'eval_runtime': 131.7779,\n"," 'eval_samples_per_second': 53.552,\n"," 'eval_steps_per_second': 6.701}"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["# Roberta-Fake-News"],"metadata":{"id":"Wv7BP-oL_YTz"}},{"cell_type":"markdown","source":["## 1.Out-of-box"],"metadata":{"id":"YjCDKt2itCT_"}},{"cell_type":"code","source":["\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","def tokenize_data(example):\n","    return tokenizer(example['title'], padding='max_length')"],"metadata":{"id":"xwoXTly9_Wly","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b94a8ba3946448a69b01c480e7c16a6f","27ecb88e32ad4b23aee006a16e380297","624e0b653d134daba688fd018d1a013c","a3d032acec384a209171f1e8917e5cb0","0188a9ed71b44abbb4a97cf97343f88e","b2ec12bdea3d422b9aef2636f5f36338","9ae23292751a4c08a03398a6c9f43e6f","bc024a7c57264777bb3e4ff987442993","120350f205fe478b9bf73aa41c5229f9","d3c9c64e53fc4a49814ba362adb80ba9","afd5cbca60e04f0791c22108369d6a93","d39d6bc3305b45bbac7cd700fc53d858","3ed9e911e6d34251ab53026bcd84a989","7fb53a41b59844acb75fa96ef2fff0df","a45e9c2303cf47b593ed773fea461175","fbf64718c95341d28b4ab82b4cb09b40","7fc3b3d8e8b04668b1f0ab1f195f0e43","3e63c88862734723993f8d7971303143","2174f03756b041f2867a924b726d088e","df8110f66d034be082d6093d6132adee","5b062dc6fbe04ef8b47ef0d8786946a5","8353d74ca1b947dc81876bc9482dd956","70207495bb9c4dc0a3bf6e9af71257da","891e3acf57c3490cb76fe15dd5802530","91aef6ae96314fffaa2697b5b3f105d3","c3ad9432db064daa8fad2ca8ba6252ae","e8f8a79cfd7d456bae62ce388a2aa675","9d92f805e39940ad91e08d5a305d2368","c49608e7f6d6438ca4e621f6efbe4f77","7eb9c09513fc41b491d7f7e7b548d06c","1eea29d6f03944b2a19b6c52a2843d56","d51291c7be95414bb2ab723ce459151f","e4bb1eedf686417a889f8ce1a5aa1da6","5c7166447b034dd5bf6c89abc47b8082","5131de01dfd847fc8451dc970e166cca","0d0095511b85452d9217045280b33a90","cc0625add6474b00a056e23e2c846bdd","efdd9ed62d8341f6b71e83cb0b5d337c","f01dc5d3d2984fcc910e477ce07004fb","c0baceded4464f3ca7e4087e13d89c2e","1e6c0f6f70d54e5e86f92574dc449489","aad5e5189ac4462b8f5290431f2c6c13","c67a39ba35974912b42966ba5bf748c3","fb7d09ff2cb4461f9a7c60601bf3b399","5ef2036da34643f78fd2fbb4937485d2","59917414af2046faaf4c3576e6267156","a42164c3a5804721b4593ee9942d0680","0092ea68b865432d976b5f59af02ced0","2b0e9cc25798469cabc01448c408da0c","59ea7f7f352c4f48a2b415433717c99e","ba401d6427cc4505a13b85df050762bc","a2ba677a54f849a8a14d81cf6d2d2d98","18b0ce097c064e8fa45b44fffb78a64d","be78983349e046fcb13656fa3cd25c90","69e2778d0a5f4e048e9238310b9575fa","ecd547b295fb4d17a397e151438e5cdd","9ddb93bfc0df47f3b04ff199f3eaacd7","be04604756bb4c8288fc899da78cd7af","8d72c3d376214203a8306026115320b7","29bc50bb883441a180e978ebaa4f52a0","b9cd5ae4862c48babf1fde0aaf8937fb","67f3a618d8f6477ca54e0d1d0c7c6bba","4e115694c5204eaa97e2c903112581d8","6eb29b6bbf654d8caf0152fe5b835bcd","a4cb363e1f1843679c1ddcdbb47a52a0","28852efc0eda416cb838e3503bed8558"]},"executionInfo":{"status":"ok","timestamp":1655585845350,"user_tz":300,"elapsed":21330,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"dfe2279e-dac1-48bc-cc3a-76bffef4b65c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8dijqi3_\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b94a8ba3946448a69b01c480e7c16a6f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/0dc2cdacd2fe1dd988f8f9976e21db42274012f2d86da28d573081dbff2a059b.71056ac4e5c419bd472fc1a3f4269ad35695eb83fe663f969180490939da9f9c\n","creating metadata file for /root/.cache/huggingface/transformers/0dc2cdacd2fe1dd988f8f9976e21db42274012f2d86da28d573081dbff2a059b.71056ac4e5c419bd472fc1a3f4269ad35695eb83fe663f969180490939da9f9c\n","https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplgup_83y\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d39d6bc3305b45bbac7cd700fc53d858"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","creating metadata file for /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4eul12nk\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70207495bb9c4dc0a3bf6e9af71257da"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d4210e7379aab0e68db3bb082412960f0b90193184d6938abedc2212cffc8ee9.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","creating metadata file for /root/.cache/huggingface/transformers/d4210e7379aab0e68db3bb082412960f0b90193184d6938abedc2212cffc8ee9.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi5vggpa9\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7166447b034dd5bf6c89abc47b8082"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/5d86bda5dc16a19ae0ebc44499ec825b06516f9aae277aff35d2ca814da2a5f1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","creating metadata file for /root/.cache/huggingface/transformers/5d86bda5dc16a19ae0ebc44499ec825b06516f9aae277aff35d2ca814da2a5f1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplf2wd5oa\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef2036da34643f78fd2fbb4937485d2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/424977d8969812c880aa638f4451597de6d4c1c429d98b7aed0822f8c327bbcb.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","creating metadata file for /root/.cache/huggingface/transformers/424977d8969812c880aa638f4451597de6d4c1c429d98b7aed0822f8c327bbcb.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d4210e7379aab0e68db3bb082412960f0b90193184d6938abedc2212cffc8ee9.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/5d86bda5dc16a19ae0ebc44499ec825b06516f9aae277aff35d2ca814da2a5f1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/424977d8969812c880aa638f4451597de6d4c1c429d98b7aed0822f8c327bbcb.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0dc2cdacd2fe1dd988f8f9976e21db42274012f2d86da28d573081dbff2a059b.71056ac4e5c419bd472fc1a3f4269ad35695eb83fe663f969180490939da9f9c\n","loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphu_10m3p\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd547b295fb4d17a397e151438e5cdd"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","creating metadata file for /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]}]},{"cell_type":"code","source":["#Dataset\n","dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)\n","\n","#Eval Parameters\n","metric = load_metric(\"accuracy\")\n","test_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"CCtNU87ewfQF","colab":{"base_uri":"https://localhost:8080/","height":400,"referenced_widgets":["6511f0a6072f4b33af472e7843859501","4312a6613b9c40288cba76688ed36f03","2a10544860d74740843da08066e08897","9709848f57494681ba950187c3424886","23539c23c9a1428094474c8163c412d3","f825a23df66b406b9ae39843ade96a94","e877c56535ad470989bcb239f2fa5dee","ef4e51c7f250492496e8b627638eea15","945feca29c3147ea9a2c0f422c4dbee8","fb2c26b6db90424e96f355ef362a062f","83e6ac54b0e642468d9599b9f38d1e47"]},"executionInfo":{"status":"ok","timestamp":1655585978276,"user_tz":300,"elapsed":132943,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"c1405691-27d7-411e-818f-fa4e42508c61"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6511f0a6072f4b33af472e7843859501"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.4545841008927306,\n"," 'eval_f1': 0.6212732460887533,\n"," 'eval_loss': 1.2726035118103027,\n"," 'eval_precision': 0.4517098297324367,\n"," 'eval_recall': 0.9946439823566477,\n"," 'eval_runtime': 130.6377,\n"," 'eval_samples_per_second': 54.02,\n"," 'eval_steps_per_second': 6.759}"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## 2.Finetuned on CoAID "],"metadata":{"id":"d7WvZAuNtVsU"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_with_shuffle.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()\n"],"metadata":{"id":"n4XGccodtXT8","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655586116224,"user_tz":300,"elapsed":137955,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"1cdc19a7-7bb5-4625-9eb6-cf7040d00a52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.9562136885362051,\n"," 'eval_f1': 0.9496660693924092,\n"," 'eval_loss': 0.2911560833454132,\n"," 'eval_precision': 0.9831365935919055,\n"," 'eval_recall': 0.9183994959042218,\n"," 'eval_runtime': 130.4395,\n"," 'eval_samples_per_second': 54.102,\n"," 'eval_steps_per_second': 6.769}"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["## 3.Fine-tuned on FNN"],"metadata":{"id":"IAFx2my9te5b"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_FNN.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"yhaU7px6tht2","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655586252324,"user_tz":300,"elapsed":136114,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"97289b3d-3f40-43be-af26-14f131f74699"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.5235935950120448,\n"," 'eval_f1': 0.5731335703402742,\n"," 'eval_loss': 1.0985732078552246,\n"," 'eval_precision': 0.48000850701829006,\n"," 'eval_recall': 0.7110901071203529,\n"," 'eval_runtime': 130.3896,\n"," 'eval_samples_per_second': 54.122,\n"," 'eval_steps_per_second': 6.772}"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## 4.Fine-tuned on CoAID&FNN"],"metadata":{"id":"1js4hvMXti2A"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_CoAID&FNN.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"mSygbfldtk1K","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655586388052,"user_tz":300,"elapsed":135737,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"a72cd939-750a-4f23-d00d-e2b0a6a5a9f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.8153606348306646,\n"," 'eval_f1': 0.7556722295143447,\n"," 'eval_loss': 0.5768745541572571,\n"," 'eval_precision': 0.9333024548402038,\n"," 'eval_recall': 0.6348456206679269,\n"," 'eval_runtime': 130.4397,\n"," 'eval_samples_per_second': 54.102,\n"," 'eval_steps_per_second': 6.769}"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## 5.Fine-tuned on CoAID&PolitiFact"],"metadata":{"id":"fCAUCJvWtllA"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_CoAID&&PolitiFact_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"GbP1Ipu0topw","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655586524882,"user_tz":300,"elapsed":136840,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"da41e71f-f88a-48a9-c3fe-52abf2fcf5d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.910726937792263,\n"," 'eval_f1': 0.8923812777587975,\n"," 'eval_loss': 0.43977585434913635,\n"," 'eval_precision': 0.9746268656716418,\n"," 'eval_recall': 0.8229363579080026,\n"," 'eval_runtime': 130.3216,\n"," 'eval_samples_per_second': 54.151,\n"," 'eval_steps_per_second': 6.776}"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## 6.Fine-tuned on CoAID & GossipCop"],"metadata":{"id":"ov-NAlfetoPk"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_CoAID&GossipCop_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"LUkSgBxXtxMU","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655586663605,"user_tz":300,"elapsed":138732,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"8e53d324-8bf9-4235-d0af-8ce04b65fe51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.8806858438429928,\n"," 'eval_f1': 0.8522288522288521,\n"," 'eval_loss': 0.3343069553375244,\n"," 'eval_precision': 0.9619651347068146,\n"," 'eval_recall': 0.7649653434152489,\n"," 'eval_runtime': 130.4747,\n"," 'eval_samples_per_second': 54.087,\n"," 'eval_steps_per_second': 6.768}"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["## 7.Fine-tuned on GossipCop"],"metadata":{"id":"3RPZBr4qtxty"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_GossipCop_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"lbSPp2cftz4o","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655586799748,"user_tz":300,"elapsed":136149,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"8b77542b-eca0-4e52-c1d3-a4be4e49a6f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.4336120164375797,\n"," 'eval_f1': 0.6029601668818914,\n"," 'eval_loss': 1.12228262424469,\n"," 'eval_precision': 0.4403017554040331,\n"," 'eval_recall': 0.9562066792690611,\n"," 'eval_runtime': 130.3712,\n"," 'eval_samples_per_second': 54.13,\n"," 'eval_steps_per_second': 6.773}"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## 8.Fine-tuned on PolitiFact"],"metadata":{"id":"Pb4yA7UZt0gD"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"ghanashyamvtatti/roberta-fake-news\")\n","\n","model.load_state_dict(torch.load(model_path+'roberta-fake-news_on_PolitiFact_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"0NpzxsWRt1h6","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655586940327,"user_tz":300,"elapsed":140586,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"04971ce1-e17c-4b5e-8617-4f8b6a2ce85b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e23f7c43b2fa5d4e963f2e3d66f0813e2f2a74e243d39756ceefbb7fd49e3db.423dc4310dc96e26b1aca95fc8c3086041fe0868cf99993dc9f737c846dca967\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"ghanashyamvtatti/roberta-fake-news\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/ghanashyamvtatti/roberta-fake-news/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/25df84fde28d696e85355be437506a664875ce39baf69ad4d961f1ae820d5c7c.6bd0b65f7890a98046e2f7a40bdf15ee81164fd1e56d307d16599a4e9af5250a\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ghanashyamvtatti/roberta-fake-news.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.6868357659061924,\n"," 'eval_f1': 0.6813725490196078,\n"," 'eval_loss': 1.3481096029281616,\n"," 'eval_precision': 0.6281233386496544,\n"," 'eval_recall': 0.7444864524259609,\n"," 'eval_runtime': 130.4345,\n"," 'eval_samples_per_second': 54.104,\n"," 'eval_steps_per_second': 6.77}"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["# Fake-News-Bert-Detect"],"metadata":{"id":"8dlR_yxJuMFe"}},{"cell_type":"markdown","source":["## 1. Out-of-box"],"metadata":{"id":"0ISNCPnlvLsl"}},{"cell_type":"code","source":["\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","def tokenize_data(example):\n","    return tokenizer(example['title'], padding='max_length')"],"metadata":{"id":"0dN8rKDTvJZa","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["be228ad6ff294808917055ef4701e351","c16db2e032564355910bfc923ff85f7e","852520aef9c44b46b0bef18e943d5031","aa86c57bcdc14366938c6db0d023aa69","0bd24129c7ec4f91a181dde8e5af60df","15d45324e855461797ddaaf8fc1a6df4","375c98399ec847f1ab245e85edd1791b","c953e903ab1641a1809e6872f79a4674","ac43b4fa05844d6896967226b8c50cae","7591245943aa4abcaac089497f5ffc0b","7ed5995ce33547678fd46e2d9363abbf","a7295b1ed25e4c4085382d85f27afeda","259fe934f1d44006ac26dfce3611f744","cab50387e209487bbdadf2a4ece16477","a5b43800429c4410ab51fcdb75d2a5b9","ffe6df3bec1e44909b8d1b48a338341a","223521de6516416c91ddb1d1198ae915","1a538b36b6b547a5a5cfe2deb63eb995","353de2cf1d7f47f3a1265ba90f1fd3f5","fce6b67d5fee4c5d80cdb586d29feaa9","8c4686aec10e40918b90d3590d45228b","a2f5cc927290403c8a0e3f8b2ed832de","b6b31cfb4f9147888bf736ffb98f6a0d","b4aa0ae98769416283cd5bd6dd24e0cc","790acb88c8994430b44e943ec92e86bb","d7e2eab7feea4ef8b18ab9443bfdaf37","77f6dda8086541988e5b42900e72fddf","0ab3470e97f54b25b488837a3f2c37ce","6ea0bb273355444ab5d92c4d42807a84","a680303e81bb4ed9acf2850daa64f05c","1e556975be9b4d0985b25abdb0591bc0","e4f8d8a1c66343d69d201050ee80a974","860b056e72a548a08e34087bfa05b622","354d4958f8474884ab81ad8aac60c76a","faea05e52e5d4fab89844f66e51df40a","224b5622cdd1482d9b036ab2c11a00bb","8726221f024b4d21b0159685426e9bfa","5e026f7f35664a85ad2eb9948f6d12e0","a78e5183a69e437585eea69933656999","98a0b27f8a2a4b76b34723126a1cdab6","c71b5445017b457bb3ce63f8c6b61ea8","c00b95ef57794ca7b98e8946394fa5fc","00fdd87c5c1647e784afd3138a60bb6a","ef06f2a6306f44c9acb6c5caa81bd838","0f7a20a13e204b32973f10d40a322665","6b98c5b8bdcb4574b7fe21154f244879","efd6489798814c04b08390e7e3f88928","012b50c01cb84118878f27b0dcc145ca","8d8141c421b644739048017dfb3dfcb0","051ee33cbff94a7b8bbb62f4b723826d","73e77319cc1a4cb7b33431801afc6a5c","6d4488ec3993495798efc4599d4d29cd","1a9f0b117dc14063b6d67e4788c80196","99943779e7ea4a57ab0c2e630e6d8bb2","4b17c1a91a0249de843b537504aeef85","b5f352de766d4d6c9e55f7d5e54897de","a9552dca948f400b842bab623f697a70","10569ed12e6749ccaa0895f13641a68f","ccb374cd03a04eefb73bebe1a5b60d14","f4134ea87e504d86a7129c392a4cb851","f401ae7c960948138fb4a2c3e1cccede","b917878d61824077bd99e4ac94ec1689","cfef79c03d454dfea4fadc098b055c4f","6d4241d68ce64882afc839fb1c007d54","0c81fbe298654f87b866c601deef10c3","41b9e789e078456a95969dfa4f5d9fac"]},"executionInfo":{"status":"ok","timestamp":1655586959754,"user_tz":300,"elapsed":19445,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"5a24538c-1a33-47c2-a3f2-80069dd917c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1uhjpj1s\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be228ad6ff294808917055ef4701e351"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/1bb953ac793c5ae15e9b5a3e1748d62cad5e9e5557408e989fb5d25f00ee6d0c.d6a300aa289d15142bf7dbad3b3a42b7c1ee1e0c874081ff0108003a2e307a2a\n","creating metadata file for /root/.cache/huggingface/transformers/1bb953ac793c5ae15e9b5a3e1748d62cad5e9e5557408e989fb5d25f00ee6d0c.d6a300aa289d15142bf7dbad3b3a42b7c1ee1e0c874081ff0108003a2e307a2a\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjz3sqcnk\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7295b1ed25e4c4085382d85f27afeda"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/5a3b85c1b5bf39e7feabb076838b178aa37c4e860c428b2fbd40ef1972f679a0.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","creating metadata file for /root/.cache/huggingface/transformers/5a3b85c1b5bf39e7feabb076838b178aa37c4e860c428b2fbd40ef1972f679a0.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgcbaxsz9\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6b31cfb4f9147888bf736ffb98f6a0d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/abccee4c0ea0661b34063ece912c47e6d578f8ab52d66c02ea3387a3b5eb79b9.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","creating metadata file for /root/.cache/huggingface/transformers/abccee4c0ea0661b34063ece912c47e6d578f8ab52d66c02ea3387a3b5eb79b9.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp533fmt3t\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"354d4958f8474884ab81ad8aac60c76a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/6a4467084bef307312ff1896f1584b5a7f28fa88d2c2c39fa2e4905e6e2bdd2c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","creating metadata file for /root/.cache/huggingface/transformers/6a4467084bef307312ff1896f1584b5a7f28fa88d2c2c39fa2e4905e6e2bdd2c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/5a3b85c1b5bf39e7feabb076838b178aa37c4e860c428b2fbd40ef1972f679a0.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/abccee4c0ea0661b34063ece912c47e6d578f8ab52d66c02ea3387a3b5eb79b9.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/6a4467084bef307312ff1896f1584b5a7f28fa88d2c2c39fa2e4905e6e2bdd2c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/1bb953ac793c5ae15e9b5a3e1748d62cad5e9e5557408e989fb5d25f00ee6d0c.d6a300aa289d15142bf7dbad3b3a42b7c1ee1e0c874081ff0108003a2e307a2a\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpukxvbrgo\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/735 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f7a20a13e204b32973f10d40a322665"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","creating metadata file for /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp22fjiip0\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5f352de766d4d6c9e55f7d5e54897de"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","creating metadata file for /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]}]},{"cell_type":"code","source":["#Dataset\n","dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)\n","\n","#Eval Parameters\n","metric = load_metric(\"accuracy\")\n","test_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"-DO6XRVfyXVu","colab":{"base_uri":"https://localhost:8080/","height":400,"referenced_widgets":["91f96c7ffcad42f79d8fc1df3e5ac280","f29979a52448481b99f549e15f993275","dae3562b1aeb40ecbd88f9933b05d7a9","bb3faea83fcf4264a53d91513b7c2388","b4ef6c7d956447a59c4849f8b1bf4a20","20888573be8d4ef6815686d4c15da8cc","8b18b0f534984af0bf7320cc2ba8a285","329ce19acf2e4640923e25d234ac2a68","7ace33fbd0864a69ac3aed4db1a5e0e5","741efd22cd8946f58407cd9521bc74e8","3e331f1e8b2746cbb4660625b0d7a1c0"]},"executionInfo":{"status":"ok","timestamp":1655587092345,"user_tz":300,"elapsed":132607,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"85cd9d73-1e0f-49d0-cf5c-92050f1e29fe"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f96c7ffcad42f79d8fc1df3e5ac280"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.7515941618251382,\n"," 'eval_f1': 0.6443497666869548,\n"," 'eval_loss': 1.211885690689087,\n"," 'eval_precision': 0.9048433048433049,\n"," 'eval_recall': 0.5003150598613737,\n"," 'eval_runtime': 130.4769,\n"," 'eval_samples_per_second': 54.086,\n"," 'eval_steps_per_second': 6.767}"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["## 2.Finetuned on CoAID"],"metadata":{"id":"L6boiGfAyE0U"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_shuffle_train_on_all_CoAID.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"dUcvvhs9yGso","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655587230143,"user_tz":300,"elapsed":137805,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"c1517511-4e85-4059-cf3a-8955cf3b6801"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.9773274762647017,\n"," 'eval_f1': 0.9746915533059158,\n"," 'eval_loss': 0.16331693530082703,\n"," 'eval_precision': 0.9787166454891995,\n"," 'eval_recall': 0.9706994328922496,\n"," 'eval_runtime': 130.5464,\n"," 'eval_samples_per_second': 54.057,\n"," 'eval_steps_per_second': 6.764}"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["## 3.Fine-tuned on FNN "],"metadata":{"id":"xT1VujLhyHEF"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_Finetune_on_FNN.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"3G1Fb7khyI1L","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655587368644,"user_tz":300,"elapsed":138507,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"32650a2c-bb3f-41f7-ce4c-b1805eac6acb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.44976618959897974,\n"," 'eval_f1': 0.6204672075065976,\n"," 'eval_loss': 0.6935955882072449,\n"," 'eval_precision': 0.44976618959897974,\n"," 'eval_recall': 1.0,\n"," 'eval_runtime': 130.4526,\n"," 'eval_samples_per_second': 54.096,\n"," 'eval_steps_per_second': 6.769}"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["## 4.Fine-tuned on CoAID&FNN "],"metadata":{"id":"m1TyH4hVyJDz"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_Finetune_on_CoAID&FNN.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"rW12njbVyK1M","colab":{"base_uri":"https://localhost:8080/","height":918},"outputId":"f87c8ae0-72d7-4cc0-d59a-7ece6a0a4a17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='580' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [580/883 01:25 < 00:44, 6.78 it/s]\n","    </div>\n","    "]},"metadata":{}}]},{"cell_type":"markdown","source":["## 5.Fine-tuned on CoAID&PolitiFact"],"metadata":{"id":"hv06SfKoyLRU"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_CoAID&PolitiFact_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"Oj8tH0zvyOHV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655587641122,"user_tz":300,"elapsed":49743,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"8aa25a2d-ce3e-4eb2-890c-67cb1c580914"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='531' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [531/883 01:17 < 00:51, 6.80 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.8961315006376648,\n"," 'eval_f1': 0.872366359045795,\n"," 'eval_loss': 0.5453846454620361,\n"," 'eval_precision': 0.9750875827170105,\n"," 'eval_recall': 0.7892249527410208,\n"," 'eval_runtime': 130.1771,\n"," 'eval_samples_per_second': 54.211,\n"," 'eval_steps_per_second': 6.783}"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["## 6.Fine-tuned on CoAID & GossipCop "],"metadata":{"id":"tZbu3DamyOlD"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_CoAID&GossipCop_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"UV2mHGs6yQE3","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655587780914,"user_tz":300,"elapsed":139794,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"03913398-045d-4a36-d17a-b46f7b397ce2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.878418591469463,\n"," 'eval_f1': 0.8520179372197311,\n"," 'eval_loss': 0.30670884251594543,\n"," 'eval_precision': 0.9413109756097561,\n"," 'eval_recall': 0.7781978575929427,\n"," 'eval_runtime': 130.8444,\n"," 'eval_samples_per_second': 53.934,\n"," 'eval_steps_per_second': 6.748}"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["## 7.Fine-tuned on GossipCop "],"metadata":{"id":"x8Mt9VxIyQYl"}},{"cell_type":"code","source":["\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_GossipCop_only.pt'))\n","\n","\n","\n","def tokenize_data(example):\n","    return tokenizer(example['title'], padding='max_length')\n","#Dataset\n","dfTotal_external = dfTotal_external.dropna()\n","dataset_external = Dataset.from_pandas(dfTotal_external)\n","dataset_external = dataset_external.map(tokenize_data, batched=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9ada4112560c40f193123ce48714aee9","4038f26e2d794226ac4badc90ffb7ef9","864da61cc7d54e8dac72735dcbdaf871","383c7c1ad3b446e0b8edef4b63115121","c1a8b5f114ad4dedbfcd03759db63c6e","3bfba27eb346483eba83cafa1b2f918d","99ae3e3e4d3a4342a07ff4be484f7b94","99db5d6e926f4734a8043d2c6e83303e","4e60eea3280f48289ae2fcd32d1c5cc3","efbbf0dbf48c44639540c2241a952156","95013866cd824ab99160898f6d600d22"]},"id":"eqpPWCMFvWxi","executionInfo":{"status":"ok","timestamp":1655588234615,"user_tz":300,"elapsed":8176,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"c8f946ff-38f5-4663-d4e1-65ba2510d4ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/5a3b85c1b5bf39e7feabb076838b178aa37c4e860c428b2fbd40ef1972f679a0.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/abccee4c0ea0661b34063ece912c47e6d578f8ab52d66c02ea3387a3b5eb79b9.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/6a4467084bef307312ff1896f1584b5a7f28fa88d2c2c39fa2e4905e6e2bdd2c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n","loading file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/1bb953ac793c5ae15e9b5a3e1748d62cad5e9e5557408e989fb5d25f00ee6d0c.d6a300aa289d15142bf7dbad3b3a42b7c1ee1e0c874081ff0108003a2e307a2a\n","loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ada4112560c40f193123ce48714aee9"}},"metadata":{}}]},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_GossipCop_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"MxkImDRtySZW","colab":{"base_uri":"https://localhost:8080/","height":378,"referenced_widgets":["61d484a9d2654b2590e79b8ce5fc2114","b62d699eff324db4bcf24b241dab3c71","7d94bcb666364b7787b2637bbc4dea4b","d5557d241b5443ab8af249c44a23ff95","e8f722eba94e46c0a36e84f271d85979","aa942a86dca94cba9401edc0d6ae7ffb","7b8bb767d66d479aa0e66858ccc958cb","938a26269910440f8459d2aca0eaba30","ccaa6825116846fc897135b3b1625b19","411827da8e44453f9495e825e99a84dc","24e03d825dde46fe92257a01f7c38c5c"]},"executionInfo":{"status":"ok","timestamp":1655588226450,"user_tz":300,"elapsed":142074,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"944d786b-831b-44c9-90a8-d18c67543089"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Parameter 'function'=<function tokenize_data at 0x7fc2ae60e8c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d484a9d2654b2590e79b8ce5fc2114"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.7515941618251382,\n"," 'eval_f1': 0.6443497666869548,\n"," 'eval_loss': 1.211885690689087,\n"," 'eval_precision': 0.9048433048433049,\n"," 'eval_recall': 0.5003150598613737,\n"," 'eval_runtime': 131.6997,\n"," 'eval_samples_per_second': 53.584,\n"," 'eval_steps_per_second': 6.705}"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## 8.Fine-tuned on PolitiFact"],"metadata":{"id":"j2Lp_veXySxA"}},{"cell_type":"code","source":["#Load saved model\n","import torch\n","model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n","\n","model.load_state_dict(torch.load(model_path+'Fake-News-Bert-Detect_finetuned_on_PolitiFact_only.pt'))\n","\n","\n","#Setup Evaluation trainer\n","trainer_eval = Trainer(\n","    model=model,\n","    args=test_args,\n","    eval_dataset=dataset_external,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Evaluate without re-train the model\n","trainer_eval.evaluate()"],"metadata":{"id":"2KCMu-3DyT6c","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655588375809,"user_tz":300,"elapsed":141197,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"55a3f3dd-a984-4b99-b96f-8d0119ed696e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/296e7242cf9d7626ddf66df9e028de52c54a42ea896a02a35a08150aeb0012dc.b3b49a6f9f8c9e38f7810f9e7f04f890283b5f9dc797e68af7ed71eb2b7e6afd\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"jy46604790/Fake-News-Bert-Detect\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/jy46604790/Fake-News-Bert-Detect/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6cef7d197f6d86a75be415374a6577454a31076c29da2583b8a441b6f273d6b0.c3091640aebed39d220ea7006739d06210601f14925e4fd5acfc493711b8081d\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at jy46604790/Fake-News-Bert-Detect.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, title. If __index_level_0__, title are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 7057\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='883' max='883' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [883/883 02:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.7551367436587785,\n"," 'eval_f1': 0.7055214723926381,\n"," 'eval_loss': 1.1625183820724487,\n"," 'eval_precision': 0.7683741648106904,\n"," 'eval_recall': 0.6521739130434783,\n"," 'eval_runtime': 130.8332,\n"," 'eval_samples_per_second': 53.939,\n"," 'eval_steps_per_second': 6.749}"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["#XLNet"],"metadata":{"id":"xcM7DDPDvEjm"}},{"cell_type":"markdown","source":["## Import XLNet tokenizer, used to convert our text into tokens that correspond to XLNet’s vocabulary."],"metadata":{"id":"KcAm5KNxkXfd"}},{"cell_type":"code","source":["import tensorflow as tf\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n","from transformers import AdamW\n","\n","from tqdm import tqdm, trange\n","import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from unicodedata import name\n","import pandas as pd\n","import glob, os\n","from datasets import Dataset\n","import numpy as np\n","from datasets import load_metric\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"Fa9mnfsg1x9A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentencepiece \n","!pip install simpletransformers"],"metadata":{"id":"gwe1mJX9k_Nc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1655588403653,"user_tz":300,"elapsed":27727,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"6f7fa432-50a0-4709-a069-b8d112c02e80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 8.9 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting simpletransformers\n","  Downloading simpletransformers-0.63.7-py3-none-any.whl (249 kB)\n","\u001b[K     |████████████████████████████████| 249 kB 9.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.3.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.8.0)\n","Collecting wandb>=0.10.32\n","  Downloading wandb-0.12.18-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 55.4 MB/s \n","\u001b[?25hCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.12.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2022.6.2)\n","Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.20.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.3.5)\n","Collecting streamlit\n","  Downloading streamlit-1.10.0-py2.py3-none-any.whl (9.1 MB)\n","\u001b[K     |████████████████████████████████| 9.1 MB 50.8 MB/s \n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.1.96)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.64.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.4.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (4.11.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (3.7.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (0.7.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.6.0->simpletransformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.9)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 72.9 MB/s \n","\u001b[?25hCollecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (7.1.2)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.17.3)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (57.4.0)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 55.4 MB/s \n","\u001b[?25hCollecting setproctitle\n","  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (5.4.8)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.15.0)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.70.13)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2022.5.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.18.0)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.5.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (21.4.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.7.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (2.0.12)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (0.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.6.0->simpletransformers) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2.8.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (1.1.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n","Collecting pydeck>=0.1.dev5\n","  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 36.4 MB/s \n","\u001b[?25hCollecting toml\n","  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n","Collecting pympler>=0.9\n","  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n","\u001b[K     |████████████████████████████████| 164 kB 65.4 MB/s \n","\u001b[?25hCollecting validators\n","  Downloading validators-0.20.0.tar.gz (30 kB)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (5.1.1)\n","Collecting blinker\n","  Downloading blinker-1.4.tar.gz (111 kB)\n","\u001b[K     |████████████████████████████████| 111 kB 59.3 MB/s \n","\u001b[?25hCollecting watchdog\n","  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 8.4 MB/s \n","\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.0)\n","Collecting rich\n","  Downloading rich-12.4.4-py3-none-any.whl (232 kB)\n","\u001b[K     |████████████████████████████████| 232 kB 74.6 MB/s \n","\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.4)\n","Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.13.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.2)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (5.7.1)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.1)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.1)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.7.0)\n","Collecting ipykernel>=5.1.2\n","  Downloading ipykernel-6.15.0-py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 66.6 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.3)\n","Collecting ipython>=7.23.1\n","  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 59.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (23.1.0)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.5)\n","Collecting jupyter-client>=6.1.12\n","  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 72.6 MB/s \n","\u001b[?25hCollecting tornado>=5.0\n","  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n","\u001b[K     |████████████████████████████████| 428 kB 63.3 MB/s \n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n","\u001b[K     |████████████████████████████████| 381 kB 60.5 MB/s \n","\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.18.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.4.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.6.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.1.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n","Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.10.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.15.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.8.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.13.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.35.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.1.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.37.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.46.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (3.3.7)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.0)\n","Building wheels for collected packages: pathtools, seqeval, blinker, validators\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=79268f0dc300517a0a36bf283cf6701f8ec0f87e5c1296d0b030aaa598e0d27a\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=8230bcb57a1ead8807fdc13a34cae300c515891c580a0230623004db8622d1f6\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=8662feab7e2d64ffce7c9939837064a12d25e4ff551842b646abf7d989ecedc3\n","  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n","  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=5b735c28ddf676f534fa8f067c4c14b6eb3efe5364516772265b7931410cb4dc\n","  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n","Successfully built pathtools seqeval blinker validators\n","Installing collected packages: tornado, prompt-toolkit, jupyter-client, ipython, ipykernel, smmap, gitdb, commonmark, watchdog, validators, toml, shortuuid, setproctitle, sentry-sdk, rich, pympler, pydeck, pathtools, GitPython, docker-pycreds, blinker, wandb, streamlit, seqeval, simpletransformers\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 5.1.1\n","    Uninstalling tornado-5.1.1:\n","      Successfully uninstalled tornado-5.1.1\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: jupyter-client\n","    Found existing installation: jupyter-client 5.3.5\n","    Uninstalling jupyter-client-5.3.5:\n","      Successfully uninstalled jupyter-client-5.3.5\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","nbclient 0.6.4 requires traitlets>=5.2.2, but you have traitlets 5.1.1 which is incompatible.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n","google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.15.0 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n","google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n","Successfully installed GitPython-3.1.27 blinker-1.4 commonmark-0.9.1 docker-pycreds-0.4.0 gitdb-4.0.9 ipykernel-6.15.0 ipython-7.34.0 jupyter-client-7.3.4 pathtools-0.1.2 prompt-toolkit-3.0.29 pydeck-0.7.1 pympler-1.0.1 rich-12.4.4 sentry-sdk-1.5.12 seqeval-1.2.2 setproctitle-1.2.3 shortuuid-1.0.9 simpletransformers-0.63.7 smmap-5.0.0 streamlit-1.10.0 toml-0.10.2 tornado-6.1 validators-0.20.0 wandb-0.12.18 watchdog-2.1.9\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","prompt_toolkit","tornado"]}}},"metadata":{}}]},{"cell_type":"code","source":["tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n"],"metadata":{"id":"-DzOWMHSkVcZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Out-of-box "],"metadata":{"id":"eohD9aF1vQG3"}},{"cell_type":"code","source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"],"metadata":{"id":"-6xlX91BvEOl","colab":{"base_uri":"https://localhost:8080/","height":180,"referenced_widgets":["7a01f4b208b44c618422bb7198f36cd6","1262fd3fc1394c5d8d5fdd07a50f0a0d","138b01af1a654589b1247e3de1a4c842","77e09f6a90464635a4eb1d1d09a614f5","3d5ced7af2c14864b673d2ad78c565b8","d8523a63fdb24ed0a4bc6fc2a2fc7a4f","43d0af299130495e92278cec0a84ddd4","7d5b452d2b1744a294ece929bc9b35f8","f6b78771d7bf4bea8acd999f589c4b21","0e8a0de9e2ac4f43ba2bbf121bc96e22","9b015486aa5949f2989b20929ecef4a5"]},"executionInfo":{"status":"ok","timestamp":1655592104531,"user_tz":300,"elapsed":15362,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"497d0cf7-8776-47c1-80d6-61e7c1b8b7cf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a01f4b208b44c618422bb7198f36cd6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"80mzkX6J2Tln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"hy7OfywR1_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"8NN1RUv62bPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"],"metadata":{"id":"Y6XjBmIk3NXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(flat_true_labels, flat_predictions)"],"metadata":{"id":"_mEZ5IFi3Nvv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592192392,"user_tz":300,"elapsed":8,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"922c9b01-0f49-4e44-876a-a32e91d63bf1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.44976618959897974"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## 2.Finetuned on CoAID "],"metadata":{"id":"exHofZHWzMX3"}},{"cell_type":"code","source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_CoAID.pt'))"],"metadata":{"id":"FzekEhhbzMBW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592261146,"user_tz":300,"elapsed":9991,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"7fb37556-3f11-479a-f065-fd8530e34815"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"BPepdGID3iYw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","model.cuda()\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"-nWALeBO3i9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"],"metadata":{"id":"0XoIbtki3lZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(flat_true_labels, flat_predictions)"],"metadata":{"id":"C8OOxgqF3l9l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592302838,"user_tz":300,"elapsed":9,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"c727854d-7039-426e-fe64-ae4eb6374701"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9436020972084456"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## 3.Fine-tuned on FNN "],"metadata":{"id":"8BDojqSVzOPC"}},{"cell_type":"code","source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_FNN.pt'))\n","model.cuda()"],"metadata":{"id":"8rfZILW6zQoB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592312039,"user_tz":300,"elapsed":9208,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"8137b919-9cbf-404f-f453-1a0e08bfca04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"q72CfOt63q8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"xgh94bkA3r_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"],"metadata":{"id":"xL9dx4Jb3tFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(flat_true_labels, flat_predictions)"],"metadata":{"id":"rnRm0MFM3uQm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592353275,"user_tz":300,"elapsed":10,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"01c14bc0-9109-4c63-d14f-b5a13366d36b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5714893014028624"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["## 4.Fine-tuned on CoAID&FNN"],"metadata":{"id":"ddhqs6nrzQIf"}},{"cell_type":"code","source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_CoAID_FNN.pt'))\n","model.cuda()"],"metadata":{"id":"0N_s5b0OzP9l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592364184,"user_tz":300,"elapsed":10916,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"6e3b46a2-3c05-4533-9c54-aef093672f57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"ot07ma9A3x2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"aGrOFzMu972k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"],"metadata":{"id":"t0TvY_Hw31uO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(flat_true_labels, flat_predictions)"],"metadata":{"id":"BUhOwEqJ37Lq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592405356,"user_tz":300,"elapsed":9,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"daa87db2-35a7-4072-95b5-93b5baf662de"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8264134901516225"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## 5.Fine-tuned on CoAID&PolitiFact "],"metadata":{"id":"uoSxQQIuzSjF"}},{"cell_type":"code","source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_CoAID_Politi.pt'))\n","model.cuda()"],"metadata":{"id":"WbWoLH9XzU_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592413827,"user_tz":300,"elapsed":8476,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"b45acb01-de72-4acb-8ea6-da528dc80c70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"uqeCyyG43yV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"gbu9xyyI9991"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"],"metadata":{"id":"qx6d5Vb432KK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(flat_true_labels, flat_predictions)"],"metadata":{"id":"dIkxS6De37tY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592455211,"user_tz":300,"elapsed":9,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"760bc83c-8c5d-44c9-feb4-555a9fb58949"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8266968966983137"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["## 6.Fine-tuned on CoAID & GossipCop "],"metadata":{"id":"a-MAXazxzVN5"}},{"cell_type":"code","source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_CoAID_Gossip.pt'))\n","model.cuda()"],"metadata":{"id":"rn-nGQKxzXNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592464595,"user_tz":300,"elapsed":9390,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"c8e2bfc4-bc3a-4ec1-b6e2-7050fc086e81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"pxUj5C2B3y7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"c9YUnXwJ9_Mf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"],"metadata":{"id":"FCqNjj5A32kN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(flat_true_labels, flat_predictions)"],"metadata":{"id":"jiAWomwo38MD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592505721,"user_tz":300,"elapsed":10,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"cf36196c-71ea-4783-b795-8e6fa112822b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8689244721553068"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["## 7.Fine-tuned on GossipCop"],"metadata":{"id":"tlxhbQGGzXnY"}},{"cell_type":"code","source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_Gossip.pt'))\n","model.cuda()"],"metadata":{"id":"uHkHb7ZFzZVU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592515220,"user_tz":300,"elapsed":9506,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"1d1cbcc8-486f-4bd5-aa08-945141ca3b47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"RKI1Cpxq3zeu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"8oNNpq9p-AHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"],"metadata":{"id":"-0XK60Ip33Cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(flat_true_labels, flat_predictions)"],"metadata":{"id":"vSskyiIW38rS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592556397,"user_tz":300,"elapsed":11,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"5ffa43da-0027-4389-98f3-7dda7ce0d255"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4160408105427235"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["## 8.Fine-tuned on PolitiFact"],"metadata":{"id":"V6YLZBBWzZk_"}},{"cell_type":"code","source":["model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.cuda()\n","model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n","model.load_state_dict(torch.load(model_path+'XLNet_on_Politi.pt'))\n","model.cuda()"],"metadata":{"id":"Ao8WW1qcza3p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592569391,"user_tz":300,"elapsed":13001,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"05ab9422-3ccf-4982-ccf6-c0cf53652f15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n","- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["XLNetForSequenceClassification(\n","  (transformer): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (6): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (7): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (8): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (9): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (10): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (11): XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (sequence_summary): SequenceSummary(\n","    (summary): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","    (first_dropout): Identity()\n","    (last_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["df = dfTotal_external\n","# Create sentence and label lists\n","sentences = df.title.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n","sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n","labels = df.label.values\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"1OwEWgvp3z-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"KWvfjq3T-BI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = [item for sublist in true_labels for item in sublist]"],"metadata":{"id":"VZB_lLJW33wM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(flat_true_labels, flat_predictions)"],"metadata":{"id":"nnZNuHWS39JD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655592610534,"user_tz":300,"elapsed":10,"user":{"displayName":"yunong liu","userId":"01039174376626853323"}},"outputId":"e58bae23-b061-4c8c-a08b-2c3ff87e2d92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7065325209012329"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["# Text-CNN"],"metadata":{"id":"-zELZJxhvGdQ"}},{"cell_type":"markdown","source":["## 1. Out-of-box 2.Finetuned on CoAID 3.Fine-tuned on FNN 4.Fine-tuned on CoAID&FNN 5.Fine-tuned on CoAID&PolitiFact 6.Fine-tuned on CoAID & GossipCop 7.Fine-tuned on GossipCop 8.Fine-tuned on PolitiFact"],"metadata":{"id":"Eh0gGG_tvRe5"}},{"cell_type":"code","source":[""],"metadata":{"id":"xo3ETrUGvIiU"},"execution_count":null,"outputs":[]}]}